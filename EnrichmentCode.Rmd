---
title: "Code for Hills, T. T. **Learning, clutter, and age-related cognitive decline: An enrichment-based account of the interdependence between fluid and crystallized intelligence**"
date: "2024-04-11"

bibliography      : /Users/thomashills/Dropbox/Life_3.0_db/Books/BNS_Hills/BNS_Github/enrichment/enrichment.bib 

floatsintext      : yes 
linenumbers       : no 
fig_caption       : yes

classoption       : "man"
toc               : false

output:  bookdown::pdf_document2
always_allow_html: true
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

```{r pressure, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
 library(igraph)
 library(tidyverse)
 library(kableExtra)
 library(latex2exp)
 library(moments)
 library(Rmisc)
 library(lsa)
```

Figure \@ref(fig:Figure1).

```{r Figure1, fig.cap="The process of translating experience with the environment into behavior. Arrows represent processes that translate one domain into another.  Learning translates experience with the environment into a cognitive representation. Additional cognitive processes then act on the representation to generate behavior."}

#### Figure 1 ####

library(DiagrammeR)


grViz(diagram = "digraph flowchart {
  graph[rankdir=LR]
  node [fontname = arial, shape = square, cex = 1, fixedsize=TRUE, width=2.3]
  tab1 [label = '@@1']
  tab2 [label = '@@2']
  tab3 [label = '@@3']
  
  tab1 -> tab2 -> tab3;
}
  
  [1]: 'Environment'
  [2]: 'Representation'    
  [3]: 'Behavior'    
  ")

```

\newpage

Figure \@ref(fig:Figure2).

```{r Figure2,fig.cap="The structure of the environment from which learning takes place. Two network types are shown: A weighted scale-free network ($a=1$) and a weighted Erd√∂s-Renyi random graph ($a=0$). Each network has 500 concepts (or nodes) and the weighted edges between them are the result of repeatedly sampling 2000 pairs of nodes and adding 1 to the edge weight between the pairs. The strength distributions (sum of the edge weights) are shown to help communicate the difference in the underlying structure.", eval = TRUE, fig.height=5.5, fig.width=5}

#### Figure 2: Environment Structure ####

# Clean and Prepare Workspace 

rm(list=ls())
set.seed(1)

# Rescorla Wagner function 

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}

# Environment Parameters #

# World parameters (500/2000/200 for)
wordsInWorld=500
# proportion
# 2000/(600*599/2) # ~1% of all edges
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200 
ageEpochs = 4

# Build Rank-Based Network Environment 

x <- 1:wordsInWorld
a = 1 # set to .1 for ranking and 0 for ER with fixed number
pairs <- c()
for(i in 1:Associates){
  pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
}

# Build ER Network Environment

x <- 1:wordsInWorld
a = 0 # set to 1 for ranking and 0 for ER with fixed number
pairser <- c()
for(i in 1:Associates){
  pairser <- rbind(pairser, sample(x, size = 2, prob=x^(-a), replace = FALSE))
}

pdf(file="Figure2.pdf")
set.seed(1)
par(mfrow=c(2,2))
par(mar=c(1,1,1,1))

# Plot Rank environment 

ii <- graph_from_edgelist(pairs,directed=FALSE) 
E(ii)$weight <- 1
ii <- graph_from_adjacency_matrix(get.adjacency(ii), weighted=TRUE, mode = "undirected")
plot(ii, vertex.size = 1, edge.arrow.size = 0, vertex.label = NA, layout = layout_with_fr(ii), main = TeX("$a = 1$"), edge.color=alpha("black", .1) )
# text(-.9,.9, TeX("$\\alpha= 1.0$"))
par(mar=c(5,5,2,2))
#plot(x=1:length(V(ii)), y=degree(ii)[order(degree(ii), decreasing = TRUE)], log="xy", ylab="Degree", xlab="Rank", pch = 16, cex = .5, ylim=c(1,1000))
# strength
plot(x=1:length(V(ii)), y=strength(ii)[order(strength(ii), decreasing = TRUE)], log="xy", ylab="Strength", xlab="Rank", pch = 16, cex = .5, col = alpha("black", .5), ylim=c(1, 1000) )

# Plot ER environment 

set.seed(1)
par(mar=c(3,3,3,3))
iier <- graph_from_edgelist(pairser,directed=FALSE)
E(iier)$weight <- 1
iier <- graph_from_adjacency_matrix(get.adjacency(iier), weighted=TRUE, mode = "undirected")
plot(iier, vertex.size = 1, edge.arrow.size = 0, vertex.label = NA, layout = layout_with_fr(iier),edge.width=E(iier)$weight, edge.color=alpha("black", .1) , main = TeX("$a = 0$"))
#text(0,-1.5, "Training network")
par(mar=c(5,5,2,2))
#plot(x=1:length(V(iier)), y=degree(iier)[order(degree(iier), decreasing = TRUE)], log="xy", ylab="Degree", xlab="Rank", pch = 16, cex = .5, col = alpha("black", .5), ylim=c(1, 20) )
# strength
plot(x=1:length(V(iier)), y=strength(iier)[order(strength(iier), decreasing = TRUE)], log="xy", ylab="Strength", xlab="Rank", pch = 16, cex = .5, col = alpha("black", .5), ylim=c(1, 20) )
dev.off()
```

\newpage

Figure \@ref(fig:Figure3).


```{r Figure3, eval = TRUE, fig.cap="Examples of the growing mental lexicon resulting from training a Rescorla-Wagner model on the network types shown in Figure 2. Training occurs in 200 event epochs, with edges from the environment sampled in proportion to their weight. Nodes represent individual concepts and edges represent learned associations. Unlearned 'isolates' are not shown in the visualization but decline with age: Across the four epochs, the number of isolates is $333$, $262$, $222$, and $191$ for $a=1$ and $239$, $123$, $63$, and $41$ for $a=0$, consistent with a rising vocabulary.", cache=TRUE}
set.seed(1)

pdf(file="Figure3.pdf")
#### Figure 3: Cognitive Representation ####

# Build Representation: Learn from Environment

# Size of Network and Learning Trials 

# a = 1

betav = .01

# Remove isolates? (set to 1 for removal or 0 for inclusion)

remiso = 1

#x <- seq(1000, 10000, 1000)
y <- rep(1000, length(x))

# Plot pars
par(mfrow=c(2,ageEpochs))
par(mar=c(2,2,2,2))


iis <- ii

# Prepare Representation Matrix 

n = length(V(iis))+1 # number of cues (words) + context cue

# initialize zero value matrix for learning
vmat <- matrix(0, nrow = n, ncol = n)
rownames(vmat) <- 1:n
colnames(vmat) <- 1:n

# initialize metrics
edgeE <- rep(NA, ageEpochs)
nodeCount <- rep(NA, ageEpochs)
ageNetworkList <- list(NULL)

# Learn Representation 

for(lage in 1:ageEpochs){
  # take fraction of environment that is learnable 
  #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
  ageWords <- wordsInWorld
  # make training data set
  traindata <- c()
  # sample edges from environment
  for(i in 1:learningEvents){
   traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
  }
  # keep list of first words learned in year 1
  if(lage == 1){
    firsttraindata <- traindata
  }
  # train on cue-outcome pairs
  for(i in 1:learningEvents){
    cue_outcome <- traindata[i,]
    cue_outcome<- sample(as.vector(cue_outcome))
    vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = betav) 
  }
 # make undirected graph from representation 
  gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
  # remove context cue
  gle <- igraph::delete_vertices(gle, n)
  nodeCount[lage] <- length(V(gle))
  # save learned representation
  ageNetworkList[[lage]] <- gle
  # copy network
  g2 <- gle
  # get symmetric weighted network 
  weightMatrix <- as_adjacency_matrix(g2, attr="weight", sparse=FALSE)
  # set negative values to zero
  weightMatrix[weightMatrix < 0] <- 0
  # normalize rows for entropy
  ww<-weightMatrix/rowSums(weightMatrix, na.rm=TRUE)
  # entropy function
  entropy <- function(p) rowSums(-(p * log(p)), na.rm = TRUE)
  # compute entropy for each node
  node_entropy <- entropy(ww) 
  
  # only compute entropy for words learned about at time 1 (removes isolates with 0 entropy)
  ftlist <- unique(c(firsttraindata[,1], firsttraindata[,2]))
  # take median entropy of list
  edgeE[lage] <- mean(node_entropy[ftlist])
  # remove edges with 0 or less weight
  g2 <- igraph::delete.edges(gle, which(E(gle)$weight <=0.0))
  # set remaining weights to 1
  E(g2)$weight <- 1
  
  # if remove isolates
  if(remiso==1){
    isolated = which(igraph::degree(g2)==0)
    #print(paste("isolates for age ", lage, " are ", length(isolated), sep = " "))
    g2 = igraph::delete.vertices(g2, isolated)  
  }
  # plot learned representation
  plot(g2, vertex.size = 1, edge.arrow.size = 0, vertex.label=NA, layout=layout_with_fr(g2, dim=3), edge.color=alpha("black", .1) )
  # label first one in 'learned'
  if(lage == 1){
    text(0, 1.5, TeX("a = 1"))
  }
  # label all with iterations
  its <- lage*learningEvents
  text(0, -1.5, paste("t =",its))
}


# Build Representation: Learn from Environment 

# Size of Network and Learning Trials 

# a = 0

#x <- seq(1000, 10000, 1000)
y <- rep(1000, length(x))

# Plot pars
par(mar=c(2,2,2,2))


iis <- iier

# Prepare Representation Matrix 

n = length(V(iis))+1 # number of cues (words) + context cue

# initialize zero value matrix for learning
vmat <- matrix(0, nrow = n, ncol = n)
rownames(vmat) <- 1:n
colnames(vmat) <- 1:n

# initialize metrics
edgeE <- rep(NA, ageEpochs)
nodeCount <- rep(NA, ageEpochs)
ageNetworkList <- list(NULL)

# Learn Representation 

for(lage in 1:ageEpochs){
  # take fraction of environment that is learnable 
  #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
  ageWords <- wordsInWorld
  # make training data set
  traindata <- c()
  # sample edges from environment
  for(i in 1:learningEvents){
   traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
  }
  # keep list of first words learned in year 1
  if(lage == 1){
    firsttraindata <- traindata
  }
  # train on cue-outcome pairs
  for(i in 1:learningEvents){
    cue_outcome <- traindata[i,]
    cue_outcome<- sample(as.vector(cue_outcome))
    vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = betav) 
  }
 # make undirected graph from representation 
  gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
  # remove context cue
  gle <- igraph::delete_vertices(gle, n)
  nodeCount[lage] <- length(V(gle))
  # Remove negative edges
  negEdges <- which(E(gle)$weight < 0)
  gle <- igraph::delete_edges(gle, negEdges)
  # save learned representation
  ageNetworkList[[lage]] <- gle
  # copy network
  g2 <- gle
  # get symmetric weighted network 
  weightMatrix <- as_adjacency_matrix(g2, attr="weight", sparse=FALSE)
  # set negative values to zero
  weightMatrix[weightMatrix < 0] <- 0
  # normalize rows for entropy
  ww<-weightMatrix/rowSums(weightMatrix, na.rm=TRUE)
  # entropy function
  entropy <- function(p) rowSums(-(p * log(p)), na.rm = TRUE)
  # compute entropy for each node
  node_entropy <- entropy(ww) 
  
  # only compute entropy for words learned about at time 1 (removes isolates with 0 entropy)
  ftlist <- unique(c(firsttraindata[,1], firsttraindata[,2]))
  # take median entropy of list
  edgeE[lage] <- mean(node_entropy[ftlist])
  # remove edges with 0 or less weight
  g2 <- igraph::delete.edges(gle, which(E(gle)$weight <=0.0))
  # set remaining weights to 1
  E(g2)$weight <- 1
  
    # if remove isolates
  if(remiso==1){
    isolated = which(igraph::degree(g2)==0)
    #print(paste("isolates for age ", lage, " are ", length(isolated), sep = " "))
    g2 = igraph::delete.vertices(g2, isolated)  
  }
  
  # plot learned representation
  plot(g2, vertex.size = 1, edge.arrow.size = 0, vertex.label=NA, layout=layout_with_fr(g2, dim=3), edge.color=alpha("black", .1) )
  # label first one in 'learned'
  if(lage == 1){
    text(0, 1.5, TeX("a = 0"))
  }
  # label all with iterations
  its <- lage*learningEvents
  text(0, -1.5, paste("t =",its))
}
dev.off()

```


\newpage

Table \@ref(tab:statsForReps).

```{r statsForReps_prep, cache=TRUE}
rm(list=ls())
set.seed(1)

par(mfrow=c(1,2))

# Rescorla Wagner function 

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}


# Environment Parameters #

# World parameters (500/2000/200 for)
wordsInWorld=500
# proportion
# 2000/(600*599/2) # ~1% of all edges
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200
ageEpochs = 4
Simsi = 2

thingstoTrack <- c("Nodes", "Edges", "Strength", "Degree","ASPL", "C" , "Mean", "Mdn", "Skewness")

# Environment data  
envdata1 <- matrix(NA, nrow = Simsi, ncol = length(thingstoTrack))
envdata0 <- matrix(NA, nrow = Simsi, ncol = length(thingstoTrack))

# Cognivitive representation data
repnodesa1<- matrix(NA, nrow = Simsi, ncol = 4)
repedgesa1<- matrix(NA, nrow = Simsi, ncol = 4)
repstrengtha1<- matrix(NA, nrow = Simsi, ncol = 4)
repdegreea1 <- matrix(NA, nrow = Simsi, ncol = 4)
repaspla1<- matrix(NA, nrow = Simsi, ncol = 4)
repca1<- matrix(NA, nrow = Simsi, ncol = 4)
repmn1<- matrix(NA, nrow = Simsi, ncol = 4)
repmdn1<- matrix(NA, nrow = Simsi, ncol = 4)
repsk1<- matrix(NA, nrow = Simsi, ncol = 4)

repnodesa0<- matrix(NA, nrow = Simsi, ncol = 4)
repedgesa0<- matrix(NA, nrow = Simsi, ncol = 4)
repstrengtha0<- matrix(NA, nrow = Simsi, ncol = 4)
repdegreea0 <- matrix(NA, nrow = Simsi, ncol = 4)
repaspla0<- matrix(NA, nrow = Simsi, ncol = 4)
repca0<- matrix(NA, nrow = Simsi, ncol = 4)
repmn0<- matrix(NA, nrow = Simsi, ncol = 4)
repmdn0<- matrix(NA, nrow = Simsi, ncol = 4)
repsk0<- matrix(NA, nrow = Simsi, ncol = 4)

for(worldsi in 1:Simsi){
  # Build Rank-Based Network Environment 
  
  x <- 1:wordsInWorld
  a = 1 # set to .1 for ranking and 0 for ER with fixed number
  pairs <- c()
  for(i in 1:Associates){
    pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
  }
  
  # Build ER Network Environment
  
  x <- 1:wordsInWorld
  a = 0 # set to 1 for ranking and 0 for ER with fixed number
  pairser <- c()
  for(i in 1:Associates){
    pairser <- rbind(pairser, sample(x, size = 2, prob=x^(-a), replace = FALSE))
  }
 
  # Make graph 
  ii <- graph_from_edgelist(pairs,directed=FALSE) 
  E(ii)$weight <- 1
  ii <- graph_from_adjacency_matrix(get.adjacency(ii), weighted=TRUE, mode = "undirected")
  
  iier <- graph_from_edgelist(pairser,directed=FALSE)
  E(iier)$weight <- 1
  iier <- graph_from_adjacency_matrix(get.adjacency(iier), weighted=TRUE, mode = "undirected")
  
  envdata1[worldsi, 1] <- sum(igraph::degree(ii, mode = "total") >= 1)
  envdata1[worldsi, 2] <- length(E(ii))
  envdata1[worldsi, 3] <- mean(igraph::strength(ii, mode = "total"))
  envdata1[worldsi, 4] <- mean(igraph::degree(ii, mode = "total"))
  envdata1[worldsi, 5] <- igraph::mean_distance(ii, unconnected=TRUE)
  envdata1[worldsi, 6] <- mean(igraph::transitivity(ii, type="local"), na.rm=TRUE)
  envdata1[worldsi, 7] <- mean(E(ii)$weight)
  envdata1[worldsi, 8] <- median(E(ii)$weight) 
  envdata1[worldsi, 9] <- skewness(E(ii)$weight)
  
  envdata0[worldsi, 1] <- sum(igraph::degree(iier, mode = "total") >= 1)
  envdata0[worldsi, 2] <- length(E(iier))
  envdata0[worldsi, 3] <- mean(igraph::strength(iier, mode = "total"))
  envdata0[worldsi, 4] <- mean(igraph::degree(iier, mode = "total"))
  envdata0[worldsi, 5] <- igraph::mean_distance(iier, unconnected=TRUE)
  envdata0[worldsi, 6] <- mean(igraph::transitivity(iier, type="local"), na.rm=TRUE)
  envdata0[worldsi, 7] <- mean(E(iier)$weight)
  envdata0[worldsi, 8] <- median(E(iier)$weight) 
  envdata0[worldsi, 9] <- skewness(E(iier)$weight)
  
  
  # Build Representation: Learn from Environment
  
  # Size of Network and Learning Trials 
  
  # a = 1
  
  betav = .01
  
  # Remove isolates? (set to 1 for removal or 0 for inclusion)
  
  remiso = 1
  
  y <- rep(1000, length(x))
  
  iis <- ii
  
  # Prepare Representation Matrix 
  
  n = length(V(iis))+1 # number of cues (words) + context cue
  
  # initialize zero value matrix for learning
  vmat <- matrix(0, nrow = n, ncol = n)
  rownames(vmat) <- 1:n
  colnames(vmat) <- 1:n
  
  
  # Learn Representation 
  
  for(lage in 1:ageEpochs){
    # take fraction of environment that is learnable 
    #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
    ageWords <- wordsInWorld
    # make training data set
    traindata <- c()
    # sample edges from environment
    for(i in 1:learningEvents){
     traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
    }
    # keep list of first words learned in year 1
    if(lage == 1){
      firsttraindata <- traindata
    }
    # train on cue-outcome pairs
    for(i in 1:learningEvents){
      cue_outcome <- traindata[i,]
      cue_outcome<- sample(as.vector(cue_outcome))
      vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = betav) 
    }
   # make undirected graph from representation 
    gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
    # remove context cue
    gle <- igraph::delete_vertices(gle, n)
    # Remove negative edges
    negEdges <- which(E(gle)$weight < 0)
    gle <- igraph::delete_edges(gle, negEdges)
    
    repnodesa1[worldsi, lage]<- sum(igraph::degree(gle, mode = "total") >= 1)
    repedgesa1[worldsi, lage] <-  length(E(gle))
    repstrengtha1[worldsi, lage] <- mean(igraph::strength(gle, mode = "total"))
    repdegreea1[worldsi, lage]  <- mean(igraph::degree(gle, mode = "total"))
    repaspla1[worldsi, lage] <-  igraph::mean_distance(gle, unconnected=TRUE)
    repca1[worldsi, lage] <- mean(igraph::transitivity(gle, type="local"), na.rm=TRUE)
    repmn1[worldsi, lage] <- mean(E(gle)$weight) 
    repmdn1[worldsi, lage] <- median(E(gle)$weight)
    repsk1[worldsi, lage] <- moments::skewness(E(gle)$weight) 
    
  }
  
  
  # Build Representation: Learn from Environment 
  
  # Size of Network and Learning Trials 
  
  # a = 0
  
  #x <- seq(1000, 10000, 1000)
  y <- rep(1000, length(x))
  
  # Plot pars
  par(mar=c(2,2,2,2))
  
  
  iis <- iier
  
  # Prepare Representation Matrix 
  
  n = length(V(iis))+1 # number of cues (words) + context cue
  
  # initialize zero value matrix for learning
  vmat <- matrix(0, nrow = n, ncol = n)
  rownames(vmat) <- 1:n
  colnames(vmat) <- 1:n
  
  # initialize metrics
  edgeE <- rep(NA, ageEpochs)
  nodeCount <- rep(NA, ageEpochs)
  ageNetworkList <- list(NULL)
  
  # Learn Representation 
  
  for(lage in 1:ageEpochs){
    # take fraction of environment that is learnable 
    #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
    ageWords <- wordsInWorld
    # make training data set
    traindata <- c()
    # sample edges from environment
    for(i in 1:learningEvents){
     traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
    }
    # keep list of first words learned in year 1
    if(lage == 1){
      firsttraindata <- traindata
    }
    # train on cue-outcome pairs
    for(i in 1:learningEvents){
      cue_outcome <- traindata[i,]
      cue_outcome<- sample(as.vector(cue_outcome))
      vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = betav) 
    }
   # make undirected graph from representation 
    gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
    # remove context cue
    gle <- igraph::delete_vertices(gle, n)
    # Remove negative edges
    negEdges <- which(E(gle)$weight < 0)
    gle <- igraph::delete_edges(gle, negEdges)
    
    # save data
    repnodesa0[worldsi, lage]<- sum(igraph::degree(gle, mode = "total") >= 1)
    repedgesa0[worldsi, lage] <-  length(E(gle))
    repstrengtha0[worldsi, lage] <- mean(igraph::strength(gle, mode = "total"))
    repdegreea0[worldsi, lage]  <- mean(igraph::degree(gle, mode = "total"))
    repaspla0[worldsi, lage] <-  igraph::mean_distance(gle, unconnected=TRUE)
    repca0[worldsi, lage] <- mean(igraph::transitivity(gle, type="local"), na.rm=TRUE)
    repmn0[worldsi, lage] <- mean(E(gle)$weight) 
    repmdn0[worldsi, lage] <- median(E(gle)$weight)
    repsk0[worldsi, lage] <- moments::skewness(E(gle)$weight) 
  }
}

a0dat <- t(matrix(c( formatC(colMeans(repnodesa0), format="f", digits = 2),
formatC(colMeans(repedgesa0), format="f", digits=1),
formatC(colMeans(repstrengtha0),format="f", digits =3),
formatC(colMeans(repdegreea0),format="f", digits = 2),
formatC(colMeans(repaspla0),format="f", digits=3),
formatC(colMeans(repca0), format="f", digits=3),
formatC(colMeans(repmn0), format="f", digits=3),
formatC(colMeans(repmdn0), format="f", digits=3),
formatC(colMeans(repsk0), format="f", digits=3)), nrow=4))

a1dat <- t(matrix(c( formatC(as.numeric(colMeans(repnodesa1), format="f", digits = 2)),
formatC(colMeans(repedgesa1), format="f", digits=1),
formatC(colMeans(repstrengtha1),format="f", digits =3),
formatC(colMeans(repdegreea1),format="f", digits = 2),
formatC(colMeans(repaspla1),format="f", digits=3),
formatC(colMeans(repca1), format="f", digits=3),
formatC(colMeans(repmn1), format="f", digits=3),
formatC(colMeans(repmdn1), format="f", digits=3),
formatC(colMeans(repsk1), format="f", digits=3)), nrow=4))

datout1 <- data.frame(colMeans(envdata0),a0dat)
names(datout1) <- c("Environment", "Epoch 1", "Epoch 2", "Epoch 3", "Epoch 4")
datout2 <- data.frame(colMeans(envdata1), a1dat)
names(datout2) <- c("Environment", "Epoch 1", "Epoch 2", "Epoch 3", "Epoch 4")

datout <- rbind(datout1, datout2)

Measures <- c(thingstoTrack, thingstoTrack )

a <- c(rep("$a=0$", 9), rep("$a=1$", 9))

datout[,1] <- round(datout[,1], 2)
datout <- cbind(a, Measures, datout)
```


```{r statsForReps}

#### Table 1 ####
datouts <- datout[-c(7:9,16:18),]
kable(datouts,row.names=FALSE, booktabs = T, escape = FALSE, caption = "Statistics for the environments and growing representations.", col.names = c("$a$", "Measures", "Environment", "1", "2", "3", "4"), font_size = 11) %>% 
  column_spec(4, border_left = TRUE)  %>%
kable_classic(full_width = F, html_font = "Cambria")   %>% 
  add_header_above(c(" " = 3, "Epoch" = 4), bold = TRUE) %>%
  row_spec(6,extra_css = "border-bottom: 1px solid;") %>% 
  collapse_rows(columns = 1) %>% 
  footnote(general = "Measures are averaged over 1000 environments and cognitive representations learned from those environments over four epochs of 200 learning events each. Nodes indicates the total number of non-isolates in the environment or cognitive representation. Strength is the sum of the edge weights. Degree is the number of associations. ASPL is the average shortest path length. C is the mean local clustering coefficient.", threeparttable = TRUE) 

```

\newpage

Figure \@ref(fig:Figure4).


```{r Figure4, fig.cap="The rising entropy and falling similarity of the aging lexicon as a function of learning. ",fig.height=3.4, cache = TRUE}

#### Figure 4 ####

# Simulation to recreate environment and cognition and entropy/similarity measures

set.seed(1)
##### Rescorla Wagner function ######

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}


# World parameters (500/2000/200 for)
wordsInWorld=500
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200 
ageEpochs = 4
Worlds = 100
Betas = seq(.01, .1, .03) # Context

alphas <- c(0,1)

Elist <- list(NULL)
Slist <- list(NULL)

for(alphai in 1:length(alphas)){
 
  EEB <- matrix(NA, nrow=length(Betas), ncol = 4)
  SSB <- matrix(NA, nrow=length(Betas), ncol = 4)
  
  for(bis in 1:length(Betas)){
    # Entropy keeper
    EE <- matrix(NA, nrow=Worlds, ncol = ageEpochs)
    # Similarity keeper
    SS <- matrix(NA, nrow=Worlds, ncol = ageEpochs)
    
    for(woi in 1:Worlds){
       #print(paste( "alpha = ", alphai, "; Beta = ", bis, "; world = ", woi, sep="")) 
      # Build the world
      x <- 1:wordsInWorld
      a = alphas[alphai] # set to 1 for ranking and 0 for ER with fixed number
      pairs <- c()
      for(i in 1:Associates){
        pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
      }
      # Make graph from pairs
      ii <- graph_from_edgelist(pairs,directed=FALSE) 
      
      # Add isolates if number of vertices is not 500
      difamt = 500-length(V(ii))
      if (difamt > 0){
       ii <-  add_vertices(ii,difamt)
      }
      
      # Rename graph for learning representation 
      iis <- ii
      
      #### Prepare Representation Matrix ####
     
      # Remove context cue 
      n = length(V(iis))  +1  # number of cues (words) + context cue
      
      # Initialize zero value matrix for learning
      vmat <- matrix(0, nrow = n, ncol = n)
      rownames(vmat) <- 1:n
      colnames(vmat) <- 1:n
      
      # Initialize metrics
      edgeE <- rep(NA, ageEpochs)
      nodeCount <- rep(NA, ageEpochs)
      ageNetworkList <- list(NULL)
      
      ##### Learn Representation #####
      
      for(lage in 1:ageEpochs){
        # take fraction of environment that is learnable 
        #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
        ageWords <- wordsInWorld
        #iis <- subgraph(ii, 1:ageWords) 
        # make training data set
        traindata <- c()
        # sample edges from environment
        for(i in 1:learningEvents){
          traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
        }
        # keep list of first words learned in year 1
        if(lage == 1){
          firsttraindata <- traindata
        }
        # train on cue-outcome pairs
        for(i in 1:learningEvents){
          cue_outcome <- traindata[i,]
          cue_outcome<- sample(as.vector(cue_outcome))
          vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = Betas[bis]) 
          # Do above without context
          #vmat <- rescorlaWagner(vmat, cue=c(cue_outcome[1]), outcome=cue_outcome[2], beta = Betas[bis]) 
        }
        # make undirected graph from representation 
        gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
        # remove context cue
        gle <- igraph::delete_vertices(gle, n)
        
        # take subgraph of learnable words (only nec. if growing)
        gle <- igraph::subgraph(gle, 1:ageWords)
        nodeCount[lage] <- length(V(gle))
        # Remove negative edges
        negativeEdges <- which(E(gle)$weight < 0)
        gle <- igraph::delete_edges(gle, negativeEdges)
        # save learned representation
        ageNetworkList[[lage]] <- gle
        # copy network
        g2 <- gle
        # get symmetric weighted network 
        weightMatrix <- as_adjacency_matrix(g2, attr="weight", sparse=FALSE)
        # set negative values to zero
        weightMatrix[weightMatrix < 0] <- 0
        # normalize rows for entropy
        ww<-weightMatrix/rowSums(weightMatrix, na.rm=TRUE)
        # entropy function
        entropy <- function(p) rowSums(-(p * log(p)), na.rm = TRUE)
        # compute entropy for each node
        node_entropy <- entropy(ww) 
        
        # only compute entropy for words learned about at time 1 (removes isolates with 0 entropy)
        ftlist <- unique(c(firsttraindata[,1], firsttraindata[,2]))
        # take median entropy of list
        edgeE[lage] <- mean(node_entropy[ftlist])
        # remove edges with 0 or less weight
        g2 <- igraph::delete.edges(gle, which(E(gle)$weight <=0.0))
        # set remaining weights to 1
        E(g2)$weight <- 1
        # plot learned representation
        # plot(g2, vertex.size = 1, edge.arrow.size = 0, vertex.label=NA, layout=layout_with_fr(g2))
        # label first one in 'learned'
        # if(lage == 1){
         #  text(0, 1.5, "Learned lexicon")
        # }
        # label all with iterations
        # its <- lage*learningEvents
        # text(0, -1.5, paste("t =",its))
      }
      # Entropy values to save 
      EE[woi,] <- edgeE
      
      #### Compute and Plot Similarity
      
      # choose target pairs, from first training data (firsttraindata)
      learningPairs = 20 # this 50 random pairs
      simpair <- firsttraindata[sample(seq(1:nrow(firsttraindata)), learningPairs ),]
      # simpair <- firsttraindata # this takes all pairs
      # set initial activation levels
      simpair <- data.frame(simpair, activation = 100)
      
      # time for spreading activation
      tspr <- 10 
      # assign column names
      names(simpair) <- c("node", "node", "activation")
      ## create datastore for similarity judgments
      simJudge <- c()
      ## for each age network
      for(sage in 1:length(ageNetworkList)){
        ## initiate activation at each node and measure activation at the other 
        for(testrow in 1:nrow(simpair)){
          # initiate activation at the cue
          df1 <- spreadr::spreadr(start_run = simpair[testrow,c(1,3)], decay = 0,
                                  retention = 0, suppress = 0,
                                  network = ageNetworkList[[sage]], time = tspr)
          # measure max activation at the target node
          maxActivation12 <- max(subset(df1, node == simpair[testrow,2])$activation)
          # initiate activation at the other cue
          df2 <- spreadr::spreadr(start_run = simpair[testrow,c(2,3)], decay = 0,
                                  retention = 0, suppress = 0,
                                  network = ageNetworkList[[sage]], time = tspr)
          # measure max activation at the target node
          maxActivation21 <- max(subset(df2, node == simpair[testrow,1])$activation)
          # add the max activations
          simval <- maxActivation12 + maxActivation21
          # add results to the data frame
          simJudge <- rbind(simJudge, c(simpair[testrow,1],simpair[testrow,2], simval, sage))
        }
      }
      # ready data frame with results
      simJudge <- data.frame(simJudge)
      # label columns
      names(simJudge) <- c("node1", "node2", "similarity", "age")
      # get stats by age
      sed <- summarySE(simJudge, measurevar="similarity", groupvars="age")
      SS[woi,] <- sed$similarity
    }
    
    msee <- apply(EE, 2, mean)
    sdee <- apply(EE, 2, sd)
    sdee <- sdee/sqrt(nrow(EE))
    mses <- apply(SS, 2, mean)
    sdes <- apply(SS, 2, sd)
    sdes <- sdes/sqrt(nrow(SS))
   
    EEB[bis,] <- msee 
    SSB[bis,] <- mses 
  }
  
Elist[[alphai]] <- EEB
Slist[[alphai]] <- SSB
}

pdf(file="Figure4.pdf", width=9, height=6)
par(mfrow=c(1,2))
plot(1:4, Elist[[1]][1,], ylim = c(0, 1.8), cex = 0, xlab = "Epoch", ylab = "Entropy", cex.lab=1.5, xaxt="n")
axis(1, at=1:4, labels=c("1","2","3","4") )
for(i in 1:nrow(Elist[[1]])){
  lines(1:4, Elist[[1]][i,], lty = i, lwd = 1.5)
}
for(i in 1:nrow(Elist[[2]])){
  lines(1:4, Elist[[2]][i,], lty = i, col = "red", lwd=1.5)
}


plot(1:4, Slist[[1]][1,], cex = 0, ylim =c(0, 150), xlab = "Epoch", ylab = "Similarity", cex.lab = 1.5, xaxt="n")
axis(1, at=1:4, labels=c("1","2","3","4") )
for(i in 1:nrow(Slist[[1]])){
  lines(1:4, Slist[[1]][i,], lty = i, lwd=1.5)
}
for(i in 1:nrow(Slist[[2]])){
  lines(1:4, Slist[[2]][i,], lty = i, col = "red", lwd=1.5)
}
legend(2.9, 150, legend=c(TeX('0'),TeX('1')), title=TeX('a'),col = c("black", "red"), lty = 1, bty="n", lwd = 1.5, cex = .9)
legend(1, 50, legend=c(TeX('.01'), TeX('.04'), TeX('.07'),TeX('.10')), title=TeX('\\beta'), lty = 1:4, bty="n", lwd=1.4, cex = .9)
dev.off()

#save(Elist, Slist, file="Figure4.RData")

```

\newpage

Figure \@ref(fig:Figure5).


```{r Figure5, cache = TRUE, echo=FALSE,out.width="100%", fig.cap="The degree, average shortest path length, and clustering coefficient for networks of associations across the lifespan. Simulations were repeated for learning cognitive representations in 1000 different environments ($a=1$) with four training epochs each of 200 learning events each($\\beta=.1$). Representations were then each sampled from by 10 simulated participants who each retrieved 3 associates for each of 30 cues with probability proportional to the associative strengths output from the Rescorla-Wagner training epochs. The cosine similarity of cues was then computed from the cue by association matrix to produce associative networks for each epoch. These networks were then thresholded separately for each learning environment by the median similarity value across all ages. Error bars indicate one standard deviation. Results are consistent with Dubossarsky et al., 2017.", fig.show='hold',fig.align='center'}

#### Figure 5 ####

set.seed(1)

# Rescorla Wagner function 

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}


# World parameters (500/2000/200 for)
wordsInWorld=500
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200 
ageEpochs = 4
Worlds =100 
# Here we fix beta at .1 and use r^-1 (alpha = 1) for rank-based network
Betas = .1
alphas <- 1
alphai = 1
bis = 1
# Data storage for network metrics by age
# trans
transdev <- matrix(NA, nrow=4, ncol=Worlds)
# degree
degreedev <- matrix(NA, nrow=4, ncol=Worlds)
# aspl 
distancedev <- matrix(NA, nrow=4, ncol=Worlds)

# Each World creates a new environment and a new developmental learning trajectory with associations
for(woi in 1:Worlds){
  # Build the world
  x <- 1:wordsInWorld
  a = alphas[alphai] # set to 1 for association network demonstration
  pairs <- c()
  for(i in 1:Associates){
    pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
  }
  # Make graph from pairs
  ii <- graph_from_edgelist(pairs,directed=FALSE) 
  
  # Add isolates if number of vertices is not 500
  difamt = 500-length(V(ii))
  if (difamt > 0){
    ii <-  add_vertices(ii,difamt)
  }
  
  # Rename graph for learning representation 
  iis <- ii
  
  # Prepare Representation Matrix 
  
  n = length(V(iis))+1 # number of cues (words) + context cue
  
  # Initialize zero value matrix for learning
  vmat <- matrix(0, nrow = n, ncol = n)
  rownames(vmat) <- 1:n
  colnames(vmat) <- 1:n
  
  # Initialize metrics
  edgeE <- rep(NA, ageEpochs)
  nodeCount <- rep(NA, ageEpochs)
  ageNetworkList <- list(NULL)
  
  # Learn Representations across epochs 
  
  for(lage in 1:ageEpochs){
    # take fraction of environment that is learnable 
    #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
    ageWords <- wordsInWorld
    # make training data set
    traindata <- c()
    # sample edges from environment
    for(i in 1:learningEvents){
      traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
    }
    # keep list of first words learned in year 1
    if(lage == 1){
      firsttraindata <- traindata
    }
    # train on cue-outcome pairs
    for(i in 1:learningEvents){
      cue_outcome <- traindata[i,]
      cue_outcome<- sample(as.vector(cue_outcome))
      vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = Betas[bis]) 
    }
    # Make undirected graph from representation 
    gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
    # Remove context cue
    gle <- igraph::delete_vertices(gle, n)
    nodeCount[lage] <- length(V(gle))
    # Remove negative edges
    negEdges <- which(E(gle)$weight < 0)
    gle <- igraph::delete_edges(gle, negEdges)
    # save learned representation
    ageNetworkList[[lage]] <- gle
  } # End development representation construction
  
  #### Compute associations for each network
  # Number of cues
  numcues = 30 
  # Make cue list from nodes with degree 3 or more across all four epochs
  deg3list.1 <- which(degree(ageNetworkList[[1]]) > 3)
  deg3list.2 <- which(degree(ageNetworkList[[2]]) > 3)
  deg3list.3 <- which(degree(ageNetworkList[[3]]) > 3)
  deg3list.4 <- which(degree(ageNetworkList[[4]]) > 3)
  # Take intersection of all lists
  gclist <- intersect(intersect(intersect(deg3list.1,deg3list.2), deg3list.3),deg3list.4)
  # Make cue list from sample of intersection of all lists
  cuelist <- sample(gclist, numcues)
  # store networks and matrices
  gcs <- list(NULL)
  gmats <- list(NULL)
  # For each age network 
  for(nits in 1:4){
    # Get weighted adjacency matrix for producing associates at each age
    getassmat <- get.adjacency(ageNetworkList[[nits]], attr="weight", sparse = FALSE)
    # Make empty cue x target matrix
    cxtm <- matrix(0, nrow= length(cuelist), ncol = length(V(ageNetworkList[[4]])))
    ## For number of participants 
    numparticipants = 10
    ## Generate up to 3 associates each 
    for(cuei in 1:length(cuelist)){
      # For number of participants
      for(partis in 1:numparticipants){
        # Sample from row of adjacency matrix in proportion to weight
        threeasss <- sample(1:ncol(cxtm), 3, prob=getassmat[cuelist[cuei],])
        # Add to cue x target matrix
        cxtm[cuei,threeasss] <- cxtm[cuei,threeasss] + 1
      } 
    }
    #### Compute cue x cue network
    cuesims <-  lsa::cosine(t(cxtm)) 
    # set diagonal to 0
    diag(cuesims) <- 0
    #### Compute stats on network
    gcs[[nits]] <- cuesims
    #### 
    gmats[[nits]] <- graph_from_adjacency_matrix(cuesims, weighted = TRUE, mode="undirected")
  } # end production of each cue x cue network across development
  
  ## compute median sim across all worlds as network threshold  
  medw <- median(unlist(gcs))
  # for each network threshold then compute values
  for(ip in 1:length(gcs)){
    nettodo <- gmats[[ip]]
    nettodo <- delete_edges(nettodo, which(E(nettodo)$weight < medw))
    E(nettodo)$weight <- 1
    # transitivity
    transdev[ip, woi] <- mean(igraph::transitivity(nettodo, type="localundirected"), na.rm=TRUE)
    # degree
    degreedev[ip, woi] <- mean(igraph::degree(nettodo))
    # aspl
    distancedev[ip, woi] <- igraph::mean_distance(nettodo)
  }
}
 
transdevalpha1 <- transdev

pdf(file="Figure5.pdf", width = 7, height = 4) 
par(mfrow=c(1,3)) 
par(mar=c(5,5,2,2))
trm <- rowMeans(transdev)
dgrm <- rowMeans(degreedev)
dirm <- rowMeans(distancedev)
sdtrm <- apply(transdev, 1, sd)
sddgrm <- apply(degreedev, 1, sd)
sddirm <- apply(distancedev, 1, sd)
plot(dgrm, cex.lab=1.5, xaxt="n", ylab = "Degree", xlab = "Epoch", ylim=c(12, 18), xlim = c(.5, 4.5))
arrows(1:4, dgrm+sddgrm, 1:4, dgrm-sddgrm, angle=90, code=3, length = .1)
axis(1, at=1:4, labels=c("1","2","3","4") )
plot(dirm, cex.lab=1.5, xaxt="n", ylab="Average shortest path length", xlab="Epoch", ylim = c(1.3, 1.6), xlim = c(.5, 4.5))
axis(1, at=1:4, labels=c("1","2","3","4") )
arrows(1:4, dirm+sddirm, 1:4, dirm-sddirm, angle=90, code=3, length = .1)
plot(trm, cex.lab=1.5, xaxt="n", ylab = "Clustering Coefficient", xlab = "Epoch", ylim = c(0.5,.8), xlim=c(.5, 4.5))
arrows(1:4, trm+sdtrm, 1:4, trm-sdtrm, angle=90, code=3, length = .1)
axis(1, at=1:4, labels=c("1","2","3","4") )
dev.off()
```

