---
title             : "A network enrichment account of cognitive aging"
shorttitle        : "Enrichment and cognitive aging"

author: 
  - name          : "Thomas Hills"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Gibbet Hill Road, Coventry, CV4 7AL, UK"
    email         : "t.t.hills@warwick.ac.uk"

affiliation:
  - id            : "1"
    institution   : "University of Warwick"

authornote: |
  This work was supported by the Alan Turing Institute and Royal Society Wolfson Research Merit Award WM160074.

abstract: |
  Late-life cognitive development is associated with a decline in fluid intelligence which occurs alongside a corresponding increase in crystallized intelligence.
  Theory often treats these accounts independently, seeing the former as a consequence of biological aging and the latter as a consequence of learning. What has not been fully explored is that lifelong learning may explain both accounts. This article describes a formal enrichment account that shows how a lifetime of experience encodes a cognitive representation that, when used to guide behavior, produces numerous quantitative effects commonly described for cognitive aging: with age,  similarity judgments between concepts fall, free associations become less predictable (higher entropy), and free association networks produced by aggregating output across associates show patterns of declining connectivity, indicated by falling network degree, rising average shortest path lengths, and a reduction in local clustering. The enrichment account demonstrates how these behavioral outcomes arise when a general prediction error model (Rescorla-Wagner) is used to train a cognitive representation through repeated experience with a structured environment and the cognitive representation is then sampled from to produce behavior (e.g., free associates or similarity judgments). This diffusion of activity and its associated consequences is a general property of network enrichment which also speaks directly to cognitive aging. Moreover, as measures of co-activation, rising entropy and falling similarity judgments provide mechanisms for cognitive slowing, explaining declining fluid intelligence. These results help better inform biological mechanisms of aging and its associated pathologies by better calibrating our predictions for accounts based on atrophy and enrichment.  
  
keywords          : "cognitive aging, Rescorla Wagner, spreading activation, network science, free associations, fluid intelligence, crystallized intelligence, cognitive slowing"

bibliography      : /Users/thomashills/Dropbox/Life_3.0_db/Books/BNS_Hills/BNS_Github/enrichment/enrichment.bib 

floatsintext      : yes 
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
always_allow_html: true
---

```{r setup, include = FALSE}
library("papaja")
#r_refs("enrichment.bib")
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
# library(DiagrammeR)
# library(tinytex)
 library(igraph)
# library(gridExtra)
# library(grid)
 library(tidyverse)
# library(igraphdata)
# library(kableExtra)
 library(latex2exp)
# library(scales)
# library(cowplot)
# library(nlme)
# library(gridGraphics)
# library(fitdistrplus)
# library(RColorBrewer)
# library(BayesFactor)
# library(ggpubr)
# library(stargazer)
 library(Rmisc)
 library(lsa)
# library(corrplot)
# library(ggraph)
# library(network)
# library(sna)
```

Cognitive aging across the adult lifespan is characterized by two distinct and well-documented patterns. As individuals age, measures of working memory, processing speed, and long-term memory show performance decrements from approximately age 20, while at the same time, measures of vocabulary and other kinds of general knowledge increase [@Salthouse:2004is; @park2009adaptive;@brysbaert2016many]. This distinction between the ability to solve novel problems in a fast and accurate way, called *fluid intelligence*, and the quantity of one's prior knowledge, called *crystallized intelligence*, is a classic division of intelligence [@cattell1987intelligence]. This division also characteristically distinguishes the old from the young.  What explains these age-related changes? Might they be explained by a single process? The evidence I provide below suggests that the answer is yes: the outcome of enriching one's cognitive representation through a lifetime of experiencing associations in the world produces an average decline in activation between any two concepts chosen at random. Before demonstrating how this effect arises, it is first useful to highlight the focal alternative accounts of aging this work addresses and to  describe the behavioral effects we hope to explain. 

# Degradation or enrichment

Cognitive aging is a rich process with evidence supporting a wide range of phenomenology and theoretical explanations [e.g., @reuter2008neurocognitive;@koen2019neural;@spreng2021exploration;@cao2014topological;@grady2006age;@Troyer:1997tt;@mata2007aging]. Much of this work is often interpreted as supporting either degradation-based accounts (e.g., involving declining frontal lobe or executive function and neural atrophy) or enrichment-based accounts (e.g., involving adaptive consequences of lifelong learning), and sometimes both. This dichotomy between things falling apart or things coming together is well characterized by the distinction between fluid and crystallized intelligence.

Declines in fluid intelligence are often seen as independent of rising crystallized intelligence. One prominant explanation for declines in fluid intelligence is the common cause theory of age-related cognitive decline, which argues that biological aging in the brain is the source of processing speed deficits [@deary2009age]. The supposition is that aging is a general process of degradation, in which factors like oxidative stress and telomere shortening damage the physiological mechanisms underpinning cognitive performance. @salthouse2013mechanisms describes some potential mechanisms: "a slower speed of transmission along single (e.g., loss of myelination) or multiple (e.g., loss of functional cells dictating circuitous linkages) pathways, or. . . delayed propagation at the connections between neural units (e.g., impairment in functioning of neurotransmitters, reduced synchronization of activation patterns)" (p. 116). Consistent with this, percent volume of grey and white-matter declines in late life [@giorgio2010age; @ge2002age]. Cortical thickness also declines alongside increases in cerebrospinal fluid space [@lemaitre2012normal]. These findings are often associated with atrophy and fit with common intuitions for biological aging---as biological things fall apart, so do the cognitive counterparts which they underpin. Moreover, neuropathology---associated with posthumously verified evidence of Alzheimer's and other neurodegenerative and cerebrovascular disease---accounts for up to 40% of the variation in late-life cognitive decline [@boyle2021degree]. Though this leaves substantial variance in cognitive-decline unexplained, it nonetheless identifies an important correspondence between biology and cognition, especially in unhealthy aging.

<!--   By these accounts, declines in fluid intelligence are a natural consequence of an enrichment of prior knowledge. This is proposed to arise either because learning strengthens prior associations which, when violated by new experiences, become harder to overcome [e.g., @ramscar2014myth] or because prior experiences 'clutter' knowledge in a way that limits the speed with which old knowledge can be accessed [@buchler2007modeling;@amer2022cluttered]. -->

<!-- There are many cognitive and brain related changes that are consistent with both degradation and enrichment accounts. For example, brain activity changes across the lifespan in relation to encoding and task processing, showing increased contributions from the default-mode network [@grady2006age]. This phenomenology is also associated with decreased modularity within brain regions combined with larger interconnectivity between regions in later life [@geerligs2015brain; @spreng2019shifting]. Spreng and Turner (2019) argue that these changes underpin a lifelong transition from exploration via fluid intelligence to exploitation focused on past knowledge [see also @spreng2021exploration]. -->

Alternatively, and far more rarely, some accounts of cognitive aging have proposed pathways suggesting an interdependence between the positive consequences of learning (crystallized intelligence) and their negative consequences for fluid intelligence. For example, @buchler2007modeling showed using simulations that if they assumed that the number of relations between concepts increased over the lifespan this would lead to more diffuse activation between concepts, analogous to a contextual fan effect. This reproduced age-related changes in recognition and familiarity. Using a different approach, @ramscar2014myth demonstrated how associations would change through repeated learning. They based their work on @desrosiers1986paired study of paired-associate learning in older adults, which found that older adults perform most poorly on stimuli that are least consistent with their prior experience. @ramscar2017mismeasurement showed that the difficulty of learning unrelated word pairs is entirely predictable from the co-occurrence frequency of those pairs in past experience. Training a Rescorla-Wagner model on typical patterns of word co-occurrences, unrelated word pairs become negatively associated over time, impairing the future learning of their association.

Still more recent work has argued for a much broader influence of age-related mental 'clutter', which may arise from representational changes across the lifespan as well as changes in executive function at the time of encoding or retrieval [@amer2022cluttered]. According to this account, processing deficits and the inability to filter out past experience can lead older adults to attend to too much information. This in turn creates processing difficulties when that information turns out to be irrelevant. For example, @li2022diachronic showed older individuals were more likely to show processing difficulties for words that changed their meaning during their lifetime. Though there is no existing formal model of clutter development, the enrichment account provided below offers one.

The different accounts describe above can be broadly categorized as degradation versus enrichment accounts, and the evidence for both is compelling.  However, to what extent we subscribe to one explanation over another should largely depend on how they work and what they explain. To my knowledge, the degradation accounts have not provided formal computational mechanisms for how degradation might lead to the observed age-related changes in healthy individuals. Presumably such mechanisms could be developed and would offer useful predictions. For example, @BorgeHolthoefer:2011bg have developed a compelling model of degradation for hyper-priming in Alzheimer's patients, but similar models for healthy aging are few and far between. In addition, formal degradation models would need to explain how degradation alters fluid intelligence but not crystallized intelligence. 

On the other hand, enrichment explanations either assume more connectivity (e.g., @buchler2007modeling) or evaluate how differential experience impairs or enhances pair-wise associations (e.g., @ramscar2014myth). As called for elsewhere [@wulff2019new], what is lacking in both cases is a full model of representational development and behavior across the adult lifespan that takes into account our understanding of the aging lexicon. This would represent a computational prediction for what we should expect from lifelong enrichment and what remains to be explained by degradation.  To benchmark such a model, we can use a number of empirical observations made over the last decade.

# The aging lexicon, entropy, and similarity

Several efforts to chart the mental lexicon across the lifespan have identified reproducible trends. @dubossarsky2017quantifying asked over 8000 people, ranging in age from roughly 10 to 70, to provide three free associates to each of 420 words. With approximately 1000 people in each age group, data was aggregated within age-groups to produce networks among the 420 words with edges representing a weighted function of common associates. After removing edges below a threshold, older networks were found to have a lower average degree (number of associations per word) higher average shortest path length (greater distance between associates), and lower average clustering coefficient (the proportion of a concept's neighbors that are themselves connected). Evidence also identified a rising entropy for associations, with associates becoming less predictable across the lifespan. Both @zortea2014graph and @wulff2022using, with varying numbers of participants and cues and barring differences in network construction methods, found similar patterns.

Analyses of memory search in older and younger adults also find consistent patterns of change in the aging mental lexicon. Using a semantic fluency task ("name all the animals you can think of"), @wulff2022structural constructed lexical networks by connecting words that appeared nearby in the lists that older and younger adults produced. This revealed that older adults' lexicons were less well-connected, similar to the patterns for free associations described above. @hills2013mechanisms modeled semantic fluency data from adults across the lifespan and found that, compared with younger adults, older adults produced strings of less similar words. Finally, @cosgrove2021quantifying used percolation analysis to investigate the resilience of older adults' mental lexicons by artificially removing connections between words and found that older adults' lexicons were less resilient to decay. These findings are all suggestive of more sparse connectivity in the outputs retrieved from the mental lexicon.

Slower judgments of similarity between words are equally consistent with the above patterns.  @wulff2022structural asked younger and older adults to judge the similarity of 77 different animals over a period of several weeks using a tablet participants took to their homes. Rating the similarity of pairs of animals on a scale from 1 to 20, @wulff2022structural found that older adults rated animals as less similar to one another than did young adults.

The key take-away with respect to what follows is that older adults produce less predictable associations (higher entropy), lower similarity judgments, and show evidence of sparsening free association networks as measured by degree, average shortest path length, and clustering coefficient. These results are intuitively consistent with representational degradation: one may imagine that a sparseness in output reflects a sparseness in the underlying representation, which is in turn caused by degradation in the neural architecture that underpins it. However, without understanding what we might expect from lifelong learning, efforts to explain cognitive aging as the result of degradation may attempt to bridge an explanatory gap that does not exist. Moreover, degradation accounts may even get the causation backwards. Which is to say, if life experience gives rise to some of the primary markers of age-related cognitive decline, then so-called atrophy in physiological structure may not be the cause of age-related cognitive decline. Rather, it may be the consequence of life-long learning. As demonstrated below, by extending standard learning and retrieval models across the lifespan,  we can predict all of the above effects as a consequence of enrichment, without the need for assuming any additional processes associated with biological aging or degradation.
 
# The enrichment account 

The enrichment account envisions behavior as the outcome of learning relationships from the environment to develop a cognitive representation, and then using this representation to generate behavior. This follows calls from previous researchers to model not only the representation but the processes that access that representation to generate behavior [@estes1975some;@johns2023scalable;@jones2015hidden;@castro2020contributions;@hills2022mind]. Critically, this account sees behavior as the outcome of process and representation, not as a direct copy of the representation. Formally, the enrichment account involves modeling three separate components:

1.  *Environment*: The environment presents the set of possible associations that could be learned.
2.  *Representation*: Learning associations from the environment generates a cognitive representation. This continues to develop across the lifespan.
3.  *Behavior*: Behavior is recovery of information from the cognitive representation appropriate to the environmental context. This generates free-associations, memory search, similarity judgments, and so on.

These stages are presented in Figure \@ref(fig:Figure1) and each are explained in detail below. The R code to reproduce the environments, learning of cognitive representations, behavior, and all figures in this manuscript are available at https://github.com/thomasthills/enrichment.

```{r Figure1, fig.cap="The process of translating environmental experiences into behaviour. Arrows represent processes that translate one domain into another.  Learning translates experience into a cognitive representation. Additional cognitive processes then act on the representation to generate behavior."}
library(DiagrammeR)

grViz(diagram = "digraph flowchart {
  graph[rankdir=LR]
  node [fontname = arial, shape = square, cex = 1, fixedsize=TRUE, width=2.3]
  tab1 [label = '@@1']
  tab2 [label = '@@2']
  tab3 [label = '@@3']
  
  tab1 -> tab2 -> tab3;
}
  
  [1]: 'Experience'
  [2]: 'Representation'    
  [3]: 'Behaviour'    
  ")
```

## Environment

The environment is represented here as network of concepts (e.g., words), with the edges (links) between concepts indicating associations that can be learned. This follows the basic idea that things in the world predict one another, and learning is here taken to address one association at a time. The qualitative pattern of behavioral results described below are not dependent on the specific network structure. To demonstrate this, I use a general framework for network construction. This is a variation of a fitness-based network model [@menczer2020first] using rank-based sampling.  Each concept is assigned a rank, $r$, from 1 to $n$. Then pairs of concepts are chosen from the lexicon each with probability $p \propto r^{-a}$ and an edge is created between them. When $a=1$, this process produces a scale-free network, which is characterized by a long-tail in which most concepts have very few associations and a few have very many.  This structure is consistent with the ubiquity of scaling laws in the cognitive sciences and the natural world [@kello2010ga;@johns2010evaluating]. When $a=0$ this process approximates a weighted Erdös-Renyi random graph, in which all nodes are connected with equal probability.

The estimate of the number of concepts people know in late life varies widely. A factor of 10 estimate is generally around 100,000 [@brysbaert2016many], though it could be much more or far less depending on one's environment [e.g., @hart1995]. The number of relationships between concepts is limited only by the network size and the human capacity to relate them. In practice, humans report only a subset of potential associations, ???. The Supplementary material demonstrates the results do not vary by size. For the purposes of demonstration and visualization, a small vocabulary is used here of $n=500$ with $2000$ pairs of concepts sampled (with replacement), weighted edges are formed between them, with the weight corresponding to the number of times the pair is chosen. The edges are weighted and non-directional (undirected). Figure \@ref(fig:Figure2) presents two example networks and their degree distributions for the different values of $a$. 

## Representation

Cognitive representations are built from experience with the environment. Here, I use the prediction error framework set out in the Rescorla-Wagner model [@rescorla1972theory] to encode properties of the environment into a cognitive representation. The choice of the Rescorla-Wagner model follows the strong evidence for learning as a process of minimizing prediction error, which is a fundamental assumption among models of reinforcement learning [@sutton2018reinforcement; @dayan2005theoretical; @mcclelland1981interactive; @hoppe2022exploration]. The Rescorla-Wagner model captures this phenomenology---including associative learning, blocking, inhibition, and extinction---and it is a model on which many subsequent models have been based [e.g., @sutton1981toward; @trimmer2012does]. Though it is not without limitations [e.g., @yau2023rescorla;@miller1995assessment], these are largely irrelevant here and the Supplementary Material shows that removing a key assumption (the compound context cue) does not alter the results. I therefore use the model to capture the generic prediction-error process inherent in the Rescorla-Wagner design and in recognition of its extreme predictive utility [e.g., @soto2023rescorla; @roesch2012surprise;@miller1995assessment].

Formally, the Rescorla-Wagner model minimizes the prediction error between the values of an observed outcome, $j$, and a cue predictive of that outcome, $i$. The value associated with the outcome is $\lambda_j$ and the value for that outcome as predicted by the cue is $V_{i \rightarrow j}$. The prediction error is the difference between them, $(\lambda_j-V_i)$, and it is minimized following each learning event according to the following rule:

$$
\Delta V_{i \rightarrow j} = \alpha_i \beta_j (\lambda_{j} - V_{i \rightarrow j})
$$

The parameter $\alpha_i$ corresponds to cue salience (some cues are easier to learn about than others) and $\beta_j$ to the learning rate (some outcomes are learned about faster than others). Both $\alpha$ and $\beta$ values are confined to values between $0$ and $1$. After learning at time $t$, the updated cue value is

$$
V_{i \rightarrow j, t+1} = V_{i \rightarrow j, t} + \Delta V_{i \rightarrow j, t}
$$

Thus, with repeated experience, $V_{i \rightarrow j}$ will approach the observed value $\lambda_j$. This follows exactly the formalization set out in prior work [@rescorla1972theory; @ramscar2017mismeasurement].  

The cognitive representation is formed by applying the Rescorla-Wagner model to the environment in the following way. Each learning event randomly samples a relationship (i.e., edge) from the environment in proportion to its weight (the number of times it is represented in the environment). Of the two concepts associated with the relationship, one is randomly assigned as the cue and the other as the outcome. The representation is then updated according to the Rescorla-Wagner model with $\alpha=1$ and $\lambda=1$. To demonstrate that the qualitative results are not dependent on the precise learning values, $\beta$ is varied from $.1$ to $.5$, which varies over the lower end of the range for the product of $\alpha$ and $\beta$ common to this model. Indeed, the results are the same for much smaller values of $\beta$, as shown in the Supplementary Material.

To simulate development, learning occurs in 4 epochs each with 200 learning trials.  The precise number of epochs or learning trials is unrelated to the qualitative pattern of results, as can be explored in the online code. Figure \@ref(fig:Figure3) provides examples of two learning representations over the four epochs for the different network types ($a=0$ and $a=1$) and  

## Results: Behavior

The stylized facts associated with cognitive aging are rising entropy, a reduction in pairwise similarity judgments, and changes in the structure of the free association network. Each of these is recovered from the representation as described below. In each case, environments are constructed independently 1000 times and then learning occurs over 4 epochs of 200 trials each.  

### Rising entropy

Rising entropy refers to the reduction in the predictability of free association targets as individuals age [@dubossarsky2017quantifying]. We can measure this using *Shannon's information entropy*. This measures the surprisingness of associates given the cue. If a cue has only a few strong associations, any given associate will be less surprising than if it has many equally weighted associations. Because the output of Rescorla-Wagner learning is a weighted edge, we can compute this for every cue in the network representation as follows:

$$
H = -\sum_{i=1}^{k}  p_i log(p_i)
$$ 

Here, $p_i$ is the proportion of the weight along edge $i$ for all $k$ edges for that cue. That is, $p_i = \frac{w_i}{\sum_k w_k}$. The entropy for the network reported below is the mean across all cues.  This is taken here to represent the long-term entropy of persons producing repeated associations for each cue. 

The result of this computation is shown in Figure \@ref(fig:Figure4).  For various network constructions and learning rates, the results consistently show that entropy rises with increased learning, following that observed in @dubossarsky2017quantifying.

### Similarity

To simulate similarity judgments, we can create a measure of co-activation between cues. To do this, we allow spreading activation to leave one node and measure activation at the other node, $A_{j \rightarrow k}$. This allows us to measure the extent to which one word co-activates the other. Doing this for both cues, we take similarity as the summed co-activation.

$$
S = A_{j \rightarrow k} + A_{j \rightarrow k}
$$

We measure this similarity for a random selection of 20 concept pairs in the representation, all of which are learned during the first epoch of learning. This ensures that individuals at each age (epoch) have experience with the concepts.

The result of this computation is shown in Figure \@ref(fig:Figure4).  For various network constructions and learning rates, the results consistently show that similarity judgments fall with increased learning, following that found in @wulff2019new.


```{r Figure2,fig.cap="The structure of the environment for two network types: a scale-free network ($a=1$) and a weighted Erdös-Renyi random graph ($a=0$). Each network has 500 concepts (or nodes) and the weighted edges between them are the result of repeatedly sampling 2000 pairs of nodes and adding 1 to the edge weight between the pairs. The degree distributions are shown to help communicate the difference in the underlying structure.", eval = TRUE, height=6, width=5}

#### Figure 2: Environment Structure ####

# Clean and Prepare Workspace 

rm(list=ls())
set.seed(1)

# Rescorla Wagner function 

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}

# Environment Parameters #

# World parameters (500/2000/200 for)
wordsInWorld=500
# proportion
# 2000/(600*599/2) # ~1% of all edges
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200 
ageEpochs = 4

# Build Rank-Based Network Environment 

x <- 1:wordsInWorld
a = 1 # set to .1 for ranking and 0 for ER with fixed number
pairs <- c()
for(i in 1:Associates){
  pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
}

# Build ER Network Environment

x <- 1:wordsInWorld
a = 0 # set to 1 for ranking and 0 for ER with fixed number
pairser <- c()
for(i in 1:Associates){
  pairser <- rbind(pairser, sample(x, size = 2, prob=x^(-a), replace = FALSE))
}

set.seed(1)
par(mfrow=c(2,2))
par(mar=c(1,1,1,1))

# Plot Rank environment 

ii <- graph_from_edgelist(pairs,directed=FALSE) 
E(ii)$weight <- 1
ii <- graph_from_adjacency_matrix(get.adjacency(ii), weighted=TRUE, mode = "undirected")
plot(ii, vertex.size = 1, edge.arrow.size = 0, vertex.label = NA, layout = layout_with_fr(ii), main = TeX("$a = 1$"), edge.color=alpha("black", .1) )
# text(-.9,.9, TeX("$\\alpha= 1.0$"))
par(mar=c(5,5,2,2))
#plot(x=1:length(V(ii)), y=degree(ii)[order(degree(ii), decreasing = TRUE)], log="xy", ylab="Degree", xlab="Rank", pch = 16, cex = .5, ylim=c(1,1000))
# strength
plot(x=1:length(V(ii)), y=strength(ii)[order(strength(ii), decreasing = TRUE)], log="xy", ylab="Strength", xlab="Rank", pch = 16, cex = .5, col = alpha("black", .5), ylim=c(1, 1000) )

# Plot ER environment 

set.seed(1)
par(mar=c(3,3,3,3))
iier <- graph_from_edgelist(pairser,directed=FALSE)
E(iier)$weight <- 1
iier <- graph_from_adjacency_matrix(get.adjacency(iier), weighted=TRUE, mode = "undirected")
plot(iier, vertex.size = 1, edge.arrow.size = 0, vertex.label = NA, layout = layout_with_fr(iier),edge.width=E(iier)$weight, edge.color=alpha("black", .1) , main = TeX("$a = 0$"))
#text(0,-1.5, "Training network")
par(mar=c(5,5,2,2))
#plot(x=1:length(V(iier)), y=degree(iier)[order(degree(iier), decreasing = TRUE)], log="xy", ylab="Degree", xlab="Rank", pch = 16, cex = .5, col = alpha("black", .5), ylim=c(1, 20) )
# strength
plot(x=1:length(V(iier)), y=strength(iier)[order(strength(iier), decreasing = TRUE)], log="xy", ylab="Strength", xlab="Rank", pch = 16, cex = .5, col = alpha("black", .5), ylim=c(1, 20) )
```


```{r Figure3, eval = TRUE, fig.cap="Examples of the growing mental lexicon resulting from training a Rescorla-Wagner model on the network types shown in Figure 2. Training occurs in 200 event epochs, with edges from the environment sampled in proportion to their weight. Nodes represent individual concepts and edges represent learned associations.", cache=TRUE}

#### Figure 3: Cognitive Representation ####

# Build Representation: Learn from Environment

# Size of Network and Learning Trials 

# a = 1

betav = .01

# Remove isolates? (set to 1 for removal or 0 for inclusion)

remiso = 1

#x <- seq(1000, 10000, 1000)
y <- rep(1000, length(x))

# Plot pars
par(mfrow=c(2,ageEpochs))
par(mar=c(2,2,2,2))


iis <- ii

# Prepare Representation Matrix 

n = length(V(iis))+1 # number of cues (words) + context cue

# initialize zero value matrix for learning
vmat <- matrix(0, nrow = n, ncol = n)
rownames(vmat) <- 1:n
colnames(vmat) <- 1:n

# initialize metrics
edgeE <- rep(NA, ageEpochs)
nodeCount <- rep(NA, ageEpochs)
ageNetworkList <- list(NULL)

# Learn Representation 

for(lage in 1:ageEpochs){
  # take fraction of environment that is learnable 
  #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
  ageWords <- wordsInWorld
  #iis <- subgraph(ii, 1:ageWords) 
  # make training data set
  traindata <- c()
  # sample edges from environment
  for(i in 1:learningEvents){
   traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
  }
  # keep list of first words learned in year 1
  if(lage == 1){
    firsttraindata <- traindata
  }
  # train on cue-outcome pairs
  for(i in 1:learningEvents){
    cue_outcome <- traindata[i,]
    cue_outcome<- sample(as.vector(cue_outcome))
    vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = betav) 
  }
 # make undirected graph from representation 
  gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
  # remove context cue
  gle <- igraph::delete_vertices(gle, n)
  # take subgraph of learnable words (only nec. if growing)
  gle <- igraph::subgraph(gle, 1:ageWords)
  nodeCount[lage] <- length(V(gle))
  # save learned representation
  ageNetworkList[[lage]] <- gle
  # copy network
  g2 <- gle
  # get symmetric weighted network 
  weightMatrix <- as_adjacency_matrix(g2, attr="weight", sparse=FALSE)
  # set negative values to zero
  weightMatrix[weightMatrix < 0] <- 0
  # normalize rows for entropy
  ww<-weightMatrix/rowSums(weightMatrix, na.rm=TRUE)
  # entropy function
  entropy <- function(p) rowSums(-(p * log(p)), na.rm = TRUE)
  # compute entropy for each node
  node_entropy <- entropy(ww) 
  
  # only compute entropy for words learned about at time 1 (removes isolates with 0 entropy)
  ftlist <- unique(c(firsttraindata[,1], firsttraindata[,2]))
  # take median entropy of list
  edgeE[lage] <- mean(node_entropy[ftlist])
  # remove edges with 0 or less weight
  g2 <- igraph::delete.edges(gle, which(E(gle)$weight <=0.0))
  # set remaining weights to 1
  E(g2)$weight <- 1
  
  # if remove isolates
  if(remiso==1){
    isolated = which(igraph::degree(g2)==0)
    g2 = igraph::delete.vertices(g2, isolated)  
  }
  # plot learned representation
  plot(g2, vertex.size = 1, edge.arrow.size = 0, vertex.label=NA, layout=layout_with_fr(g2, dim=3), edge.color=alpha("black", .1) )
  # label first one in 'learned'
  if(lage == 1){
    text(0, 1.5, TeX("a = 1"))
  }
  # label all with iterations
  its <- lage*learningEvents
  text(0, -1.5, paste("t =",its))
}


# Build Representation: Learn from Environment 

# Size of Network and Learning Trials 

# a = 0

#x <- seq(1000, 10000, 1000)
y <- rep(1000, length(x))

# Plot pars
par(mar=c(2,2,2,2))


iis <- iier

# Prepare Representation Matrix 

n = length(V(iis))+1 # number of cues (words) + context cue

# initialize zero value matrix for learning
vmat <- matrix(0, nrow = n, ncol = n)
rownames(vmat) <- 1:n
colnames(vmat) <- 1:n

# initialize metrics
edgeE <- rep(NA, ageEpochs)
nodeCount <- rep(NA, ageEpochs)
ageNetworkList <- list(NULL)

# Learn Representation 

for(lage in 1:ageEpochs){
  # take fraction of environment that is learnable 
  #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
  ageWords <- wordsInWorld
  #iis <- subgraph(ii, 1:ageWords) 
  # make training data set
  traindata <- c()
  # sample edges from environment
  for(i in 1:learningEvents){
   traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
  }
  # keep list of first words learned in year 1
  if(lage == 1){
    firsttraindata <- traindata
  }
  # train on cue-outcome pairs
  for(i in 1:learningEvents){
    cue_outcome <- traindata[i,]
    cue_outcome<- sample(as.vector(cue_outcome))
    vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = betav) 
  }
 # make undirected graph from representation 
  gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
  # remove context cue
  gle <- igraph::delete_vertices(gle, n)
  # take subgraph of learnable words (only nec. if growing)
  gle <- igraph::subgraph(gle, 1:ageWords)
  nodeCount[lage] <- length(V(gle))
  # Remove negative edges
  negEdges <- which(E(gle)$weight < 0)
  gle <- igraph::delete_edges(gle, negEdges)
  # save learned representation
  ageNetworkList[[lage]] <- gle
  # copy network
  g2 <- gle
  # get symmetric weighted network 
  weightMatrix <- as_adjacency_matrix(g2, attr="weight", sparse=FALSE)
  # set negative values to zero
  weightMatrix[weightMatrix < 0] <- 0
  # normalize rows for entropy
  ww<-weightMatrix/rowSums(weightMatrix, na.rm=TRUE)
  # entropy function
  entropy <- function(p) rowSums(-(p * log(p)), na.rm = TRUE)
  # compute entropy for each node
  node_entropy <- entropy(ww) 
  
  # only compute entropy for words learned about at time 1 (removes isolates with 0 entropy)
  ftlist <- unique(c(firsttraindata[,1], firsttraindata[,2]))
  # take median entropy of list
  edgeE[lage] <- mean(node_entropy[ftlist])
  # remove edges with 0 or less weight
  g2 <- igraph::delete.edges(gle, which(E(gle)$weight <=0.0))
  # set remaining weights to 1
  E(g2)$weight <- 1
  
    # if remove isolates
  if(remiso==1){
    isolated = which(igraph::degree(g2)==0)
    g2 = igraph::delete.vertices(g2, isolated)  
  }
  
  # plot learned representation
  plot(g2, vertex.size = 1, edge.arrow.size = 0, vertex.label=NA, layout=layout_with_fr(g2, dim=3), edge.color=alpha("black", .1) )
  # label first one in 'learned'
  if(lage == 1){
    text(0, 1.5, TeX("a = 0"))
  }
  # label all with iterations
  its <- lage*learningEvents
  text(0, -1.5, paste("t =",its))
}


```

```{r Fig4ForSaving, fig.cap="The rising entropy and falling similarity of the aging lexicon as a function of learning. ",fig.height=3.4, eval = FALSE, cache = TRUE}

#### Figure 4: Similarity and Entropy ####

# Simulation to recreate environment and cognition and entropy/similarity measures

set.seed(1)

# Rescorla Wagner function 

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}


# World parameters (500/2000/200 for)
wordsInWorld=500
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200 
ageEpochs = 4
Worlds =50 
Betas = seq(.1,.5, .1)

alphas <- c(0,1)

Elist <- list(NULL)
Slist <- list(NULL)

for(alphai in 1:length(alphas)){
 
  EEB <- matrix(NA, nrow=length(Betas), ncol = 4)
  SSB <- matrix(NA, nrow=length(Betas), ncol = 4)
  
  for(bis in 1:length(Betas)){
    # Entropy keeper
    EE <- matrix(NA, nrow=Worlds, ncol = ageEpochs)
    # Similarity keeper
    SS <- matrix(NA, nrow=Worlds, ncol = ageEpochs)
    
    for(woi in 1:Worlds){
        
      # Build the world
      x <- 1:wordsInWorld
      a = alphas[alphai] # set to 1 for ranking and 0 for ER with fixed number
      pairs <- c()
      for(i in 1:Associates){
        pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
      }
      # Make graph from pairs
      ii <- graph_from_edgelist(pairs,directed=FALSE) 
      
      # Add isolates if number of vertices is not wordsInWorld 
      difamt = wordsInWorld-length(V(ii))
      if (difamt > 0){
       ii <-  add_vertices(ii,difamt)
      }
      
      # Rename graph for learning representation 
      iis <- ii
      
      # Prepare Representation Matrix 
      
      n = length(V(iis))+1 # number of cues (words) + context cue
      
      # Initialize zero value matrix for learning
      vmat <- matrix(0, nrow = n, ncol = n)
      rownames(vmat) <- 1:n
      colnames(vmat) <- 1:n
      
      # Initialize metrics
      edgeE <- rep(NA, ageEpochs)
      nodeCount <- rep(NA, ageEpochs)
      ageNetworkList <- list(NULL)
      
      # Learn Representation 
      
      for(lage in 1:ageEpochs){
        # take fraction of environment that is learnable 
        #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
        ageWords <- wordsInWorld
        #iis <- subgraph(ii, 1:ageWords) 
        # make training data set
        traindata <- c()
        # sample edges from environment
        for(i in 1:learningEvents){
          traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
        }
        # keep list of first words learned in year 1
        if(lage == 1){
          firsttraindata <- traindata
        }
        # train on cue-outcome pairs
        for(i in 1:learningEvents){
          cue_outcome <- traindata[i,]
          cue_outcome<- sample(as.vector(cue_outcome))
          vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = Betas[bis]) 
        }
        # make undirected graph from representation 
        gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
        # remove context cue
        gle <- igraph::delete_vertices(gle, n)
        # take subgraph of learnable words (only nec. if growing)
        gle <- igraph::subgraph(gle, 1:ageWords)
        nodeCount[lage] <- length(V(gle))
        # save learned representation
        ageNetworkList[[lage]] <- gle
        # copy network
        g2 <- gle
        # get symmetric weighted network 
        weightMatrix <- as_adjacency_matrix(g2, attr="weight", sparse=FALSE)
        # set negative values to zero
        weightMatrix[weightMatrix < 0] <- 0
        # normalize rows for entropy
        ww<-weightMatrix/rowSums(weightMatrix, na.rm=TRUE)
        # entropy function
        entropy <- function(p) rowSums(-(p * log(p)), na.rm = TRUE)
        # compute entropy for each node
        node_entropy <- entropy(ww) 
        
        # only compute entropy for words learned about at time 1 (removes isolates with 0 entropy)
        ftlist <- unique(c(firsttraindata[,1], firsttraindata[,2]))
        # take median entropy of list
        edgeE[lage] <- mean(node_entropy[ftlist])
        # remove edges with 0 or less weight
        g2 <- igraph::delete.edges(gle, which(E(gle)$weight <=0.0))
        # set remaining weights to 1
        E(g2)$weight <- 1
        # plot learned representation
        # plot(g2, vertex.size = 1, edge.arrow.size = 0, vertex.label=NA, layout=layout_with_fr(g2))
        # label first one in 'learned'
        # if(lage == 1){
         #  text(0, 1.5, "Learned lexicon")
        # }
        # label all with iterations
        # its <- lage*learningEvents
        # text(0, -1.5, paste("t =",its))
      }
      # Entropy values to save 
      EE[woi,] <- edgeE
      
      #### Compute and Plot Similarity
      
      # choose target pairs, from first training data (firsttraindata)
      learningPairs = 20 # this 50 random pairs
      simpair <- firsttraindata[sample(seq(1:nrow(firsttraindata)), learningPairs ),]
      # simpair <- firsttraindata # this takes all pairs
      # set initial activation levels
      simpair <- data.frame(simpair, activation = 100)
      
      # time for spreading activation
      tspr <- 10 
      # assign column names
      names(simpair) <- c("node", "node", "activation")
      ## create datastore for similarity judgments
      simJudge <- c()
      ## for each age network
      for(sage in 1:length(ageNetworkList)){
        ## initiate activation at each node and measure activation at the other 
        for(testrow in 1:nrow(simpair)){
          # initiate activation at the cue
          df1 <- spreadr::spreadr(start_run = simpair[testrow,c(1,3)], decay = 0,
                                  retention = 0, suppress = 0,
                                  network = ageNetworkList[[sage]], time = tspr)
          # measure max activation at the target node
          maxActivation12 <- max(subset(df1, node == simpair[testrow,2])$activation)
          # initiate activation at the other cue
          df2 <- spreadr::spreadr(start_run = simpair[testrow,c(2,3)], decay = 0,
                                  retention = 0, suppress = 0,
                                  network = ageNetworkList[[sage]], time = tspr)
          # measure max activation at the target node
          maxActivation21 <- max(subset(df2, node == simpair[testrow,1])$activation)
          # add the max activations
          simval <- maxActivation12 + maxActivation21
          # add results to the data frame
          simJudge <- rbind(simJudge, c(simpair[testrow,1],simpair[testrow,2], simval, sage))
        }
      }
      # ready data frame with results
      simJudge <- data.frame(simJudge)
      # label columns
      names(simJudge) <- c("node1", "node2", "similarity", "age")
      # get stats by age
      sed <- summarySE(simJudge, measurevar="similarity", groupvars="age")
      SS[woi,] <- sed$similarity
    }
    
    msee <- apply(EE, 2, mean)
    sdee <- apply(EE, 2, sd)
    sdee <- sdee/sqrt(nrow(EE))
    mses <- apply(SS, 2, mean)
    sdes <- apply(SS, 2, sd)
    sdes <- sdes/sqrt(nrow(SS))
   
    EEB[bis,] <- msee 
    SSB[bis,] <- mses 
  }
  
Elist[[alphai]] <- EEB
Slist[[alphai]] <- SSB
}

#pdf(file="EntropySim.pdf", width=9, height=6)
par(mfrow=c(1,2))
plot(1:4, Elist[[1]][1,], ylim = c(0, 2.7), cex = 0, xlab = "Epoch", ylab = "Entropy", cex.lab=1.5, xaxt="n")
axis(1, at=1:4, labels=c("1","2","3","4") )
for(i in 1:nrow(Elist[[1]])){
  lines(1:4, Elist[[1]][i,], lty = i, lwd = 1.5)
}
for(i in 1:nrow(Elist[[2]])){
  lines(1:4, Elist[[2]][i,], lty = i, col = "red", lwd=1.5)
}


plot(1:4, Slist[[1]][1,], cex = 0, ylim =c(0, 80), xlab = "Epoch", ylab = "Similarity", cex.lab = 1.5, xaxt="n")
axis(1, at=1:4, labels=c("1","2","3","4") )
for(i in 1:nrow(Slist[[1]])){
  lines(1:4, Slist[[1]][i,], lty = i, lwd=1.5)
}
for(i in 1:nrow(Slist[[2]])){
  lines(1:4, Slist[[2]][i,], lty = i, col = "red", lwd=1.5)
}
legend(2.8, 80, legend=c(TeX('0'),TeX('1')), title=TeX('\\alpha'),col = c("black", "red"), lty = 1, bty="n", lwd = 1.5 )
legend(2.8, 60, legend=c(TeX('.1'), TeX('.2'), TeX('.3'),TeX('.4'),TeX('.5')), title=TeX('\\beta'), lty = 1:5, bty="n", lwd=1.5)
#dev.off()
```

```{r Figure4, echo=FALSE,out.width="100%", fig.cap="Entropy and similarity measures computed from the cognitive representation at different epochs of development.",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("EntropySim1000.1to.5.pdf"))
``` 

# Free association networks 

As noted above, @dubossarsky2017quantifying and others found that free association networks collected across the adult lifespan showed patterns of decreasing network degree, increasing average shortest path length, and decreasing clustering coefficient. We simulate this on the cognitive representations across the four epochs as follows. For each cognitive representation, we simulate 10 participants who retrieve 3 associates from each of 30 cues. The three associates are sampled without replacement for each participant and each cue is sampled in proportion to the associative strength encoded in the cognitive representation, i.e., the output from the Rescorla-Wagner training epochs. Negative association strengths are set to 0.  This produces a cue by associate matrix, with each cell indicating the number of times each associate was produced in response to each cue. From this matrix, cue-by-cue similarities are computed using cosine similarity of their corresponding associate vectors.  To transform these into unweighted and undirected networks, the median cue-by-cue similarity is computed across all epochs and, for each age, cue similarities below this median value are set to 0, and all other values are set to 1. This matrix is then transformed into a network on which degree (the number of connections), average shortest path length (the shortest number of edges between cues), and clustering coefficient (the proportion of a node's neighbors that are themselves connected) are computed. This entire process is repeated for 1000 different initial starting environments. The results are shown in Figure \@ref(fig:Figure5). The results follow the pattern found by @dubossarsky2017quantifying and others. 

```{r assnet, eval=FALSE}
# Sample free associations for each cue, then produce cue x cue matrix (cosine similarity) and get back networks of cues to compute transitivity, degree, aspl

# Simulation to recreate environment and cognition and entropy/similarity measures

set.seed(1)
# Rescorla Wagner function 

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}


# World parameters (500/2000/200 for)
wordsInWorld=500
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200 
ageEpochs = 4
Worlds =1000 
# Here we fix beta at .1 and use r^-1 (alpha = 1) for rank-based network
Betas = .1
alphas <- 1
alphai = 1
bis = 1
# Data storage for network metrics by age
# trans
transdev <- matrix(NA, nrow=4, ncol=Worlds)
# degree
degreedev <- matrix(NA, nrow=4, ncol=Worlds)
# aspl 
distancedev <- matrix(NA, nrow=4, ncol=Worlds)

# Each World creates a new environment and a new developmental learning trajectory with associations
for(woi in 1:Worlds){
  # Build the world
  x <- 1:wordsInWorld
  a = alphas[alphai] # set to 1 for association network demonstration
  pairs <- c()
  for(i in 1:Associates){
    pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
  }
  # Make graph from pairs
  ii <- graph_from_edgelist(pairs,directed=FALSE) 
  
  # Add isolates if number of vertices is not 500
  difamt = 500-length(V(ii))
  if (difamt > 0){
    ii <-  add_vertices(ii,difamt)
  }
  
  # Rename graph for learning representation 
  iis <- ii
  
  # Prepare Representation Matrix 
  
  n = length(V(iis))+1 # number of cues (words) + context cue
  
  # Initialize zero value matrix for learning
  vmat <- matrix(0, nrow = n, ncol = n)
  rownames(vmat) <- 1:n
  colnames(vmat) <- 1:n
  
  # Initialize metrics
  edgeE <- rep(NA, ageEpochs)
  nodeCount <- rep(NA, ageEpochs)
  ageNetworkList <- list(NULL)
  
  # Learn Representations across epochs 
  
  for(lage in 1:ageEpochs){
    # take fraction of environment that is learnable 
    #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
    ageWords <- wordsInWorld
    #iis <- subgraph(ii, 1:ageWords) 
    # make training data set
    traindata <- c()
    # sample edges from environment
    for(i in 1:learningEvents){
      traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
    }
    # keep list of first words learned in year 1
    if(lage == 1){
      firsttraindata <- traindata
    }
    # train on cue-outcome pairs
    for(i in 1:learningEvents){
      cue_outcome <- traindata[i,]
      cue_outcome<- sample(as.vector(cue_outcome))
      vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = Betas[bis]) 
    }
    # Make undirected graph from representation 
    gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
    # Remove context cue
    gle <- igraph::delete_vertices(gle, n)
    # take subgraph of learnable words (only nec. if growing)
    gle <- igraph::subgraph(gle, 1:ageWords)
    nodeCount[lage] <- length(V(gle))
    # Remove negative edges
    negEdges <- which(E(gle)$weight < 0)
    gle <- igraph::delete_edges(gle, negEdges)
    # save learned representation
    ageNetworkList[[lage]] <- gle
  } # End development representation construction
  
  #### Compute associations for each network
  # Number of cues
  numcues = 30 
  # Make cue list from nodes with degree 3 or more across all four epochs
  deg3list.1 <- which(degree(ageNetworkList[[1]]) > 3)
  deg3list.2 <- which(degree(ageNetworkList[[2]]) > 3)
  deg3list.3 <- which(degree(ageNetworkList[[3]]) > 3)
  deg3list.4 <- which(degree(ageNetworkList[[4]]) > 3)
  # Take intersection of all lists
  gclist <- intersect(intersect(intersect(deg3list.1,deg3list.2), deg3list.3),deg3list.4)
  # Make cue list from sample of intersection of all lists
  cuelist <- sample(gclist, numcues)
  # store networks and matrices
  gcs <- list(NULL)
  gmats <- list(NULL)
  # For each age network 
  for(nits in 1:4){
    # Get weighted adjacency matrix for producing associates at each age
    getassmat <- get.adjacency(ageNetworkList[[nits]], attr="weight", sparse = FALSE)
    # Make empty cue x target matrix
    cxtm <- matrix(0, nrow= length(cuelist), ncol = length(V(ageNetworkList[[4]])))
    ## For number of participants 
    numparticipants = 10
    ## Generate up to 3 associates each 
    for(cuei in 1:length(cuelist)){
      # For number of participants
      for(partis in 1:numparticipants){
        # Sample from row of adjacency matrix in proportion to weight
        threeasss <- sample(1:ncol(cxtm), 3, prob=getassmat[cuelist[cuei],])
        # Add to cue x target matrix
        cxtm[cuei,threeasss] <- cxtm[cuei,threeasss] + 1
      } 
    }
    #### Compute cue x cue network
    cuesims <-  lsa::cosine(t(cxtm)) 
    # set diagonal to 0
    diag(cuesims) <- 0
    #### Compute stats on network
    gcs[[nits]] <- cuesims
    #### 
    gmats[[nits]] <- graph_from_adjacency_matrix(cuesims, weighted = TRUE, mode="undirected")
  } # end production of each cue x cue network across development
  
  ## compute median sim across all worlds as network threshold  
  medw <- median(unlist(gcs))
  # for each network threshold then compute values
  for(ip in 1:length(gcs)){
    nettodo <- gmats[[ip]]
    nettodo <- delete_edges(nettodo, which(E(nettodo)$weight < medw))
    E(nettodo)$weight <- 1
    # transitivity
    transdev[ip, woi] <- mean(igraph::transitivity(nettodo, type="localundirected"), na.rm=TRUE)
    # degree
    degreedev[ip, woi] <- mean(igraph::degree(nettodo))
    # aspl
    distancedev[ip, woi] <- igraph::mean_distance(nettodo)
  }
}
  
pdf(file="NetworkAssociations1000.pdf", width = 7, height = 4) 
par(mfrow=c(1,3)) 
par(mar=c(5,5,2,2))
trm <- rowMeans(transdev)
dgrm <- rowMeans(degreedev)
dirm <- rowMeans(distancedev)
sdtrm <- apply(transdev, 1, sd)
sddgrm <- apply(degreedev, 1, sd)
sddirm <- apply(distancedev, 1, sd)
plot(dgrm, cex.lab=1.5, xaxt="n", ylab = "Degree", xlab = "Epoch", ylim=c(12, 18), xlim = c(.5, 4.5))
arrows(1:4, dgrm+sddgrm, 1:4, dgrm-sddgrm, angle=90, code=3, length = .1)
axis(1, at=1:4, labels=c("1","2","3","4") )
plot(dirm, cex.lab=1.5, xaxt="n", ylab="Average shortest path length", xlab="Epoch", ylim = c(1.3, 1.6), xlim = c(.5, 4.5))
axis(1, at=1:4, labels=c("1","2","3","4") )
arrows(1:4, dirm+sddirm, 1:4, dirm-sddirm, angle=90, code=3, length = .1)
plot(trm, cex.lab=1.5, xaxt="n", ylab = "Clustering Coefficient", xlab = "Epoch", ylim = c(0.5,.8), xlim=c(.5, 4.5))
arrows(1:4, trm+sdtrm, 1:4, trm-sdtrm, angle=90, code=3, length = .1)
axis(1, at=1:4, labels=c("1","2","3","4") )
dev.off()
```

```{r Figure5, echo=FALSE,out.width="100%", fig.cap="The degree, average shortest path length, and clustering coefficient for networks of associations across the lifespan. Simulations were repeated for learning cognitive representations in 1000 different environments ($a=1$) with four training epochs each of 200 learning events each($beta=.1$). Representations were then each sampled from by 10 simulated participants who each retrieved 3 associates for each of 30 cues with probability proportional to the associative strengths output from the Rescorla-Wagner training epochs. The cosine similarity of cues was then computed from the cue by association matrix to produce associative networks for each epoch. These networks were then thresholded separately for each learning environment by the median similarity value across all ages. Error bars indicate one standard deviation. Results are consistent with Dubossarsky et al., 2017.", fig.show='hold',fig.align='center'}
knitr::include_graphics(c("NetworkAssociations1000.pdf"))
``` 



# Discussion

Aging is marked by complex and multi-faceted phenomenology. The enrichment account provided here demonstrates that a subset of this phenomenology is predicted by cognitive enrichment. Specifically, the enrichment account demonstrates how learning enriches the cognitive representation and then, using various processes, gives rise to behavior commonly associated with cognitive aging.  In particular, enrichment reproduces rising entropy, falling similarity judgments, and a sparsening of the free association network across the lifespan. 

The enrichment account is not meant to be a complete explanation of cognitive aging.  Rather, like any model, it puts guardrails on our ignorance. It does this by showing how many of the behaviors that might be intuitively interpreted as evidence of cognitive and neural degradation are straightforward outcomes of learning. Whereas we might have suspected degradation, enrichment may be the more likely candidate, especially given that we know enrichment is already occurring in association with crystallized intelligence. Moreover, degradation can produce the opposite effect.  @BorgeHolthoefer:2011bg used degradation *not* to demonstrate diffusion of activation and greater slowing, but rather to demonstrate an increasing of activation between associates, producing hyper-priming, a well-documented behavior in patients with Alzheimer's Disease. Nonetheless, the patterns of biological and neural change outlined in the introduction are themselves data, and any full account of cognitive aging must take this evidence into account. Ideally, we want understand how these changes in neural architecture interact with enrichment to produce the behavioral variation that we see. With respect to parsimony, enrichment may be sufficient to explain how fluid intelligence might change as a result of changes in crystallized intelligence (as opposed to treating each independently), but our theories must also be parsimonious with respect to observed changes in the underlying neural architecture.  

In addition, the enrichment account is incomplete for several other reasons. One key extension is that lifelong experience with language follows Herdan-Heap's law [@brysbaert2016many]. Herdan-Heaps' law is the observation that the number of new word types is an increasing function of the number of word tokens, and this appears to be true for human experience with language as well. This is easily added to the enrichment account by allowing the environment to grow over time. Future work should investigate various implications of this addition. A second limitation is that the enrichment account is not meant to characterize child development, which has special developmental features of its own. For example, early vocabulary acquisition is driven by a variety of processes including semantics, phonology, social pragmatics, and perceptual features [@jimenez2022semantic;@ciaglia2023investigating;@fourtassi2020growth;@yu2019infant;@engelthaler2017feature]. Moreover, this different pattern of acquisition may extend into young adulthood [@dubossarsky2017quantifying]. Extending our understanding of the development of the cognitive representation across the lifespan remains for future work.

Despite these limitations, the fundamental mechanism of the enrichment account is supported empirically. For example, the fan effect demonstrates that learning many relationships with a target concept reduces the speed of accessing any one of those relationships [@anderson1999fan]. This diffusion of activation is a general and well-understood outcome of spreading activation or random walker models [@siew2019spreadr;@abbott2015random]. Activation follows pathways and the more pathways there are the less targeted the activation. In addition, one can see this effect in natural experiments: @ramscar2017mismeasurement showed that individuals with more language experience (native speakers) were more impaired in paired-associative learning in that language than age-matched individuals with less experience (second language learners). This suggests the effect is not about age, but about enrichment.  

We also know that external clutter influences processing speed in older adults more strongly than young adults [@mccarley2012age;@amer2022cluttered]. This is consistent with higher conceptual entropy creating a more level-playing field for competition. If there is greater associative competition in older adults, because the associative entropy is higher, then older adults will suffer more from clutter in internal and external processing. External cues gain their relevance via their encoded associations in memory [@easdale2019onset].  The attentional contribution to clutter on aging memory, as elegantly described by @amer2022cluttered, is therefore explained by the enrichment account: the activation strengths associated with extraneous and potentially irrelevant information have a greater relative competitive advantage in individuals with more enriched representations, making older individuals more susceptible to internal and external distractions.

One challenge to the enrichment account is that individuals with higher education and occupational attainment are less likely to experience late-life cognitive decline [@stern1994influence;@lovden2020education]. This is known to be partially a consequence of compensation accorded by skills or strategic repertoires, what is called cognitive reserve [@scarmeas2003cognitive]. Additionally, however, there is ample evidence that differences in cognitive skills emerge early in life, prior to education, and therefore lay the foundation for educational attainment later in life [@lovden2020education;@deary2004impact]. If differences in processing speed at an early age influence educational attainment at a later age, then slowing later as a consequence of education will be confounded by early differences in processing. Early processing advantages may not be a result of experience at all. Brain volume is correlated with IQ [@pietschnig2022differing] and is strongly heritable [@peper2007genetic]. Furthermore, measures of 'brain age' in late life are well-predicted by indicators present in early life [@vidal2021individual]. Nonetheless, if we believe that formal education (or occupational attainment) is an indicator of increased associative density (not just in quality, but quantity), then an ideal test might be to investigate twins who differ in educational enrichment and are then evaluated on age-related cognitive decline. The @ramscar2017mismeasurement study on age-matched bilinguals and monolinguals is another approach.  What kinds of education matter for enrichment is an open question.  Is rewatching the 11 seasons of the 1970s TV series M\*A\*S\*H  (approximately 125 hours runtime) different from reading the complete works of Gabriel Garcia Marquez for the first time?  If enrichment is about the variety of associations, and not the quality of associations (if such quality could be objectively measured), then the difference may not matter. 

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
