---
title             : "An enrichment account of cognitive aging"
shorttitle        : "Enrichment and cognitive aging"

author: 
  - name          : "Thomas Hills"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Gibbet Hill Road, Coventry, CV4 7AL, UK"
    email         : "t.t.hills@warwick.ac.uk"

affiliation:
  - id            : "1"
    institution   : "University of Warwick"

authornote: |
  This work was supported by the Alan Turing Institute and Royal Society Wolfson Research Merit Award WM160074.

abstract: |
  Late-life cognitive development is associated with a decline in fluid intelligence which occurs alongside a corresponding increase in crystallized intelligence.
  Theory often treats these accounts independently---seeing the former as a consequence of biological aging and the latter as a natural consequence of learning. What has not been formally explored is that the lifelong learning may explain both accounts. This article describes a formal enrichment account of learning across the lifespan that shows how standard reinforcement learning exposed to a lifetime of associative learning can produce two quantitative effects recently described for cognitive aging: with age, free associations become less predictable (higher entropy) and similarity judgments fall.
  The enrichment account assumes that individuals learn a cognitive representation through repeated experience with a structured environment. They then sample from that representation using spreading activation to produce associates and make similarity judgements. This reliably produces produces rising entropy and falling similarity judgments across a range of possible network types and learning assumptions.  Moreover, as measures of co-activation, rising entropy and falling similarity provide mechanisms for cognitive slowing. Thus, the enrichment account shows how crystallized intelligence and fluid intelligence are intertwined and both consequences of an enriched cognitive representation.
  
keywords          : "cognitive aging, Rescorla Wagner, spreading activation, network science, "

bibliography      : /Users/thomashills/Dropbox/Life_3.0_db/Books/BNS_Hills/BNS_Github/enrichment/enrichment.bib 

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
always_allow_html: true
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
# library(DiagrammeR)
# library(tinytex)
 library(igraph)
# library(gridExtra)
# library(grid)
 library(tidyverse)
# library(igraphdata)
# library(kableExtra)
 library(latex2exp)
# library(scales)
# library(cowplot)
# library(nlme)
# library(gridGraphics)
# library(fitdistrplus)
# library(RColorBrewer)
# library(BayesFactor)
# library(ggpubr)
# library(stargazer)
 library(Rmisc)
# library(corrplot)
# library(ggraph)
# library(network)
# library(sna)
```

Cognitive aging across the adult lifespan is characterized by two distinct and well-document patterns. As individuals age measures of working memory, processing speed, and long-term memory show apparent performance decrements from approximately the age of 20, while at the same time, measures of vocabulary and other kinds of general knowledge increase [@Salthouse:2004is; @park2009adaptive]. This distinction between the ability to solve novel problems in a fast and accurate way, called *fluid intelligence*, and the quantity of one's prior knowledge, called *crystallized intelligence*, is a classic division of intelligence [@cattell1987intelligence]. This division also stereotypically distinguishes the old from the young. What explains these age-related changes? Two prominent accounts have been proposed.

# Degradation and enrichment

Declines in fluid intelligence are often seen as independent of rising crystallized intelligence. For example, the common cause theory of age-related cognitive decline argues that biological aging in the brain is the source of processing speed deficits [@deary2009age]. The supposition is that aging is a general process of degradation, in which factors like oxidative stress and telomere shortening damage the physiological mechanisms underpinning cognitive performance. @salthouse2013mechanisms illustrates some potential mechanisms: "a slower speed of transmission along single (e.g., loss of myelination) or multiple (e.g., loss of functional cells dictating circuitous linkages) pathways, or. . . delayed propagation at the connections between neural units (e.g., impairment in functioning of neurotransmitters, reduced synchronization of activation patterns)" (p. 116). Consistent with this, percent volume of grey and white-matter declines in late life [@giorgio2010age; @ge2002age]. Cortical thickness also declines alongside increases in cerebrospinal fluid space [@lemaitre2012normal]. These findings are often associated with atrophy and fit with common intuitions for biological aging---things fall apart. Moreover, neuropathology---associated with posthumously verified evidence of Alzheimer's and other neurodegenerative and cerebrovascular disease---accounts for up to 40% of the variation in late-life cognitive decline [@boyle2021degree]. Though this leaves substantial variance in cognitive-decline unexplained, it nonetheless identifies an important correspondence between biology and cognition.

<!--   By these accounts, declines in fluid intelligence are a natural consequence of an enrichment of prior knowledge. This is proposed to arise either because learning strengthens prior associations which, when violated by new experiences, become harder to overcome [e.g., @ramscar2014myth] or because prior experiences 'clutter' knowledge in a way that limits the speed with which old knowledge can be accessed [@buchler2007modeling;@amer2022cluttered]. -->

<!-- There are many cognitive and brain related changes that are consistent with both degradation and enrichment accounts. For example, brain activity changes across the lifespan in relation to encoding and task processing, showing increased contributions from the default-mode network [@grady2006age]. This phenomenology is also associated with decreased modularity within brain regions combined with larger interconnectivity between regions in later life [@geerligs2015brain; @spreng2019shifting]. Spreng and Turner (2019) argue that these changes underpin a lifelong transition from exploration via fluid intelligence to exploitation focused on past knowledge [see also @spreng2021exploration]. -->

Alternatively, several accounts of cognitive aging have proposed various pathways for an interdependence between crystallized and fluid intelligence. @buchler2007modeling showed using simulations that if the number of relations between concepts increased over the lifespan this would lead to more diffuse activation between concepts, analogous to a contextual fan effect. This they argued could reproduce age-related changes in recognition and familiarity. Though @buchler2007modeling assumed that associations would increase, @ramscar2014myth used a learning model to demonstrate how associations would change through repeated learning. They based their work on @desrosiers1986paired study of paired-associate learning in older adults, which found that older adults perform most poorly on stimuli that are least consistent with their prior experience. @ramscar2017mismeasurement showed that the difficulty of learning unrelated word pairs is entirely predictable from the frequency of co-occurrence of those words. Training a Rescorla-Wagner model on typical patterns of word co-occurrences, unrelated word pairs become negatively associated over time, which would impair learning of these concepts later.

Still more recent work has argued for a much broader influence of age-related mental 'clutter', which may arise from representational changes across the lifespan as well as changes in executive function at the time of encoding or retrieval [@amer2022cluttered]. According to this account, processing deficits and the inability to filter out past experience can lead older adults to attend to too much information. This in turn creates processing difficulties because some of this information is irrelevant, but also offers older adults benefits when 'irrelevant' information becomes relevant.

The different accounts describe above can be broadly categorized as degradation versus enrichment accounts, and the evidence for both is compelling.  However, to what extent we subscribe to one explanation over another is largely dependent on our understanding how these mechanisms might work. To my knowledge, the atrophy accounts have not provided formal computational mechanisms for how degradation might lead to the observed age-related changes in healthy individuals. Presumably such mechanisms could be developed and would offer useful predictions. For example, @BorgeHolthoefer:2011bg have developed a compelling model of degradation for hyper-priming in Alzheimer's patients. In addition, formal degradation models would need to explain how these mechanisms alter fluid intelligence but not crystallized intelligence. On the other hand, enrichment explanations either assume more connectivity (e.g., @buchler2007modeling) or evaluate how differential experience impairs or enhances pair-wise associations (e.g., @ramscar2014myth). As called for elsewhere [@wulff2019new], what is lacking in both cases is a full model of representational development across the adult lifespan that takes into account our understanding of the aging lexicon. This could in turn be used as a computational benchmark for understanding what we should expect from enrichment alone and what remains to be explained by degradation.  In particular, two recent findings offer themselves as challenges.

# Entropy and similarity in the aging lexicon

Several efforts to chart the mental lexicon across the lifespan using free associates have revealed reproducible trends in late life. @dubossarsky2017quantifying asked over 8000 people, ranging in age from roughly 10 to 70, to provide three free associates to each of 420 words. With approximately 1000 people in each age group, data was aggregated within age-groups to produce networks among the 420 words with edges representing a weighted function of common associates. After thresholding the networks by removing edges below a threshold value, older networks were found to have a lower average degree (number of associations per word) and higher average shortest path length (greater distance between associates). Notably, this was underpinned by a rising entropy for associations, such that associations became less predictable across the lifespan. @zortea2014graph found a similar pattern with a smaller group of participants. With a still smaller group of participants (n=8) but far more cues (n=3000), @wulff2022using found this pattern yet again.

Analyses of memory search in older and younger adults also find consistent patterns of change in aging mental lexicons. Using a semantic fluency task ("name all the animals you can think of"), @wulff2022using constructed lexical networks by connecting words that appeared nearby in the lists that older and younger adults produced and found that older adults' lexicons were less well-connected. Similarly, @hills2013mechanisms modeled the semantic fluency task using semantic space models and found that, compared with younger adults, older adults produced strings of less similar words. Finally, @cosgrove2021quantifying used percolation analysis to investigate the resilience of older adults' mental lexicons by artificially removing connections between words and found that older adults' lexicons were less resilient to decay. These findings are all suggestive of more sparse connectivity in the outputs retrieved from the mental lexicon.

Judgements of similarity between words are equally consistent with the above patterns. Compared with younger adults, older adults judge animals to be less similar to one another. @wulff2022structural asked younger and older adults to judge the similarity of 77 different animals over a period of several weeks using a tablet participants could take home with them. Rating the similarity of pairs of animals on a scale from 1 to 20, @wulff2022structural found that older adults tended to rate animals as less similar than young adults.

The key take-away with respect to what follows is that older adults produce less predictable associations (higher entropy) and lower similarity judgments than younger adults. These results are intuitively consistent with representational degradation, as we may imagine that a sparseness in output reflects a sparseness in the underlying representation. However, without understanding what we might expect from lifelong learning, efforts to explain cognitive aging as the result of degradation may attempt to bridge an explanatory gap that does not exist. Moreover, they may even get the causation backwards. Which is to say, if life experience gives rise to some of the primary markers of age-related cognitive decline, then so-called atrophy in physiological structure may not be the cause of age-related cognitive decline, but the consequence of life-long learning. As demonstrated below, by extending standard learning and retrieval models across the lifespan,  we can predict both of these effects as a consequence of enrichment, without the need for assuming any additional processes associated with biological aging or degradation.
 
# The enrichment model

The enrichment account envisions behavior as the outcome of learning relationships from the environment to develop a cognitive representation, and then using this representation to generate behavior. This follows calls from previous researchers to model not only the representation but the processes by which that representation are accessed [@estes1975some;@johns2023scalable;@jones2015hidden]. Thus, the enrichment account involves modeling three separate components:

1.  *Environment*: The environment presents the set of possible experiences, i.e., associations.
2.  *Representation*: Learning from the environment generates a cognitive representation. This continues to develop across the lifespan.
3.  *Behavior*: Behavior is recovery of information from the cognitive representation appropriate to the environmental context. This generates free-associations, memory search, similarity judgments, and so on.

These stages are presented in Figure \@ref(fig:ERR) and each are explained in detail below.

```{r Figure1, fig.cap="The process of translating environmental experiences into behaviour. Arrows represent processes that translate one domain into another.  Learning translates experience into a cognitive representation. Additional cognitive processes then act on the representation to generate behavior."}
library(DiagrammeR)

grViz(diagram = "digraph flowchart {
  graph[rankdir=LR]
  node [fontname = arial, shape = square, cex = 1, fixedsize=TRUE, width=2.3]
  tab1 [label = '@@1']
  tab2 [label = '@@2']
  tab3 [label = '@@3']
  
  tab1 -> tab2 -> tab3;
}
  
  [1]: 'Experience'
  [2]: 'Representation'    
  [3]: 'Behaviour'    
  ")
```

## Environment

The environment presents the set of possible relationships an individual could learn. It is represented here as network of concepts (e.g., words), with the edges between concepts indicating learnable associations. The environment presented here is a variation of fitness-based network model using rank-based sampling. This is inspired by the ubiquity of scaling laws in the cognitive sciences and the natural world [@Kello2010ga]. This assumes a power-law relation in the frequency of concepts: Each concept is assigned a rank, $r$, from 1 to 1000. Then pairs of concepts are chosen from the lexicon each with probability $p \propto r^{-a}$ and an edge is created between them. Here $a$ is set to $.1$ and 2000 edges are created. A scale-free network is not required to get the aging results shown below---for example, an Erdös-Renyi random graph produces the same qualitative pattern. In addition, the number of words available to learn could increase across the lifespan, as proposed by @brysbaert2016many and following *Herdan-Heaps' law* [@Serrano:2009cv; @petersen2012languages]. Again, the qualitative results are the same. Examples are provided in the Supplementary material.

## Representation

To build a cognitive representation from experiences with the environment, I use the basic prediction error framework set out in the Rescorla-Wagner model. This follows the strong evidence for learning as a process of minimizing prediction error, which is a fundamental assumption among models of reinforcement learning [@sutton2018reinforcement; @dayan2005theoretical; @mcclelland1981interactive; @hoppe2022exploration]. The Rescorla-Wagner model [@rescorla1972theory] captures this phenomenology---including associative learning, blocking, inhibition, and extinction---and it is a model on which many subsequent models have been based [e.g., @sutton1981toward; @trimmer2012does]. Though it is not without criticism [@yau2023rescorla], I use the model here to capture the basic prediction-error framework inherent in its design.

Formally, the Rescorla-Wagner model minimizes the prediction error between the values of an observed outcome, $\lambda_j$ and a cue predictive of that outcome, $V_{i \rightarrow j}$, where $j$ and $i$ represent specific outcomes and cues, respectively. The prediction error is the difference between them, $(\lambda_j-V_i)$, and it is minimized following each learning event according to the following rule:

$$
\Delta V_{i \rightarrow j} = \alpha_i \beta_j (\lambda_{j} - V_{i \rightarrow j})
$$

$\alpha_i$ corresponds to cue salience (some cues are easier to learn about than others) and $\beta_j$ to the learning rate (some outcomes are learned about faster than others). Both $\alpha$ and $\beta$ values are confined to values between $0$ and $1$. After learning at time $t$, the updated cue value is

$$
V_{i \rightarrow j, t+1} = V_{i \rightarrow j, t} + \Delta V_{i \rightarrow j, t}
$$

This follows the formalizations set out in prior work [@rescorla1972theory; @ramscar2017mismeasurement].  

The cognitive representation is formed by applying the Rescorla-Wagner model to the environment in the following way. Each learning event randomly samples a relationship from the environment represented as an edge (or association) in the environment network. Of the two concepts at each end of the edge, one is randomly assigned as the cue and the other as the outcome. The representation is then updated according to the Rescorla-Wagner model with $\alpha=1$ and $\beta=.2$ and $\lambda_i=1$. 

## Behaviour

The two stylized facts associated with cognitive aging are rising entropy and a reduction in pairwise similarity judgments. Each of these is recovered from the representation as follows.

### Rising entropy

Rising entropy refers to the reduction in the predictability of free association targets as individuals age [@dubossarsky2017quantifying]. We can measure this using *Shannon's information entropy*. This measures the surprisingness of associate given the presence of a cue. Because the output of Rescorla-Wagner learning is a weighted edge, we can compute this for every cue in the network representation as follows:

$$
H = -\sum_{i=1}^{k}  p_i log(p_i)
$$ Here, $p_i$ is the proportion of the weight along edge $i$ for all $k$ edges. That is, $p_i = \frac{w_i}{\sum_k w_k}$.

### Similarity

To simulate similarity judgments, we will create a measure of co-activation between cues. To do this, we will allowing spreading activation to leave one node and measuring activation at the other node, $A_{j \rightarrow k}$. This allows us to measure the extent to which one word co-activates the other. Doing this for both cues, we take similarity as the summed co-activation.

$$
S = A_{j \rightarrow k} + A_{j \rightarrow k}
$$

We will measure this similarity for all node pairs in the representation.

# Results

```{r fig1,fig.cap="The structure of the environment.", eval = TRUE}
#### Clean and Prepare Workspace #####

rm(list=ls())
set.seed(1)

##### Rescorla Wagner function ######

rescorlaWagner <- function(vmat = vmat, cue, outcome, alpha=1, beta=.2) {
  # cue can be compound: e.g., cue = c("A", "B")
  # outcome can be compound: e.g., outcome = c("A", "B")
  cuevalues <- matrix(vmat[cue, ], nrow = length(cue))
  valueSums <- apply(cuevalues,2,sum )     # cumulative cue value
  # lambda = 1 when present 0 when absent
  lambda = as.numeric(rownames(vmat) %in% outcome)
  pError <- lambda - valueSums     # prediction error = observed - expected
  valueChange <- alpha * beta * pError # value change 
  vmat[cue, ] <- t(t(vmat[cue, ]) + valueChange)                   # update value
  return(vmat)
}

##### Environment Parameters #####

# World parameters (500/2000/200 for)
wordsInWorld=500
Associates = 2000 # Edges in environment
# Set learning events and age classes
learningEvents <- 200 
ageEpochs = 4

#### Build Rank-Based Network Environment #####

x <- 1:wordsInWorld
a = 1 # set to .1 for ranking and 0 for ER with fixed number
pairs <- c()
for(i in 1:Associates){
  pairs <- rbind(pairs, sample(x, size = 2, prob=x^(-a), replace = FALSE))
}

#### Build ER Network Environment #####

x <- 1:wordsInWorld
a = 0 # set to 1 for ranking and 0 for ER with fixed number
pairser <- c()
for(i in 1:Associates){
  pairser <- rbind(pairser, sample(x, size = 2, prob=x^(-a), replace = FALSE))
}

set.seed(1)
par(mfrow=c(2,2))
par(mar=c(3,3,3,3))
#### Plot Rank environment #####

ii <- graph_from_edgelist(pairs,directed=FALSE) 
#E(ii)$weight <- 1
#ii <- graph_from_adjacency_matrix(get.adjacency(ii), weighted=TRUE, mode = "undirected")
plot(ii, vertex.size = 1, edge.arrow.size = 0, vertex.label = NA, layout = layout_with_fr(ii),main = TeX("$\\alpha= 1$"))
# text(-.7,.9, TeX("$\\alpha= 1.0$"))
par(mar=c(5,5,2,2))
plot(x=1:length(V(ii)), y=degree(ii)[order(degree(ii), decreasing = TRUE)], log="xy", ylab="Degree", xlab="Rank", pch = 16, cex = .5)
#text(200,90, TeX("$\\alpha= 1.0$"), cex = 1.5)

#### Plot ER environment #####

set.seed(1)
par(mar=c(3,3,3,3))
iier <- graph_from_edgelist(pairser,directed=FALSE) 
E(iier)$weight <- 1
iier <- graph_from_adjacency_matrix(get.adjacency(iier), weighted=TRUE, mode = "undirected")
plot(iier, vertex.size = 1, edge.arrow.size = 0, vertex.label = NA, layout = layout_with_fr(iier),edge.width=E(iier)$weight, main = TeX("$\\alpha= 0$"))
text(0,-1.5, "Training network")
par(mar=c(5,5,2,2))
plot(x=1:length(V(iier)), y=degree(iier)[order(degree(iier), decreasing = TRUE)], log="xy", ylab="Degree", xlab="Rank", pch = 16, cex = .5, col = alpha("black", .5) )
#text(-10,8, TeX("$\\alpha= 0$"), cex = 1.5, xpd=NA)
```


```{r fig2, eval = TRUE, fig.cap="The growing mental lexicon. "}
#### Build Representation: Learn from Environment ####

##### Size of Network and Learning Trials #####

#x <- seq(1000, 10000, 1000)
y <- rep(1000, length(x))

# Plot pars
par(mfrow=c(2,ageEpochs))
par(mar=c(2,2,2,2))


iis <- ii

#### Prepare Representation Matrix ####

n = length(V(iis))+1 # number of cues (words) + context cue

# initialize zero value matrix for learning
vmat <- matrix(0, nrow = n, ncol = n)
rownames(vmat) <- 1:n
colnames(vmat) <- 1:n

# initialize metrics
edgeE <- rep(NA, ageEpochs)
nodeCount <- rep(NA, ageEpochs)
ageNetworkList <- list(NULL)

##### Learn Representation #####

for(lage in 1:ageEpochs){
  # take fraction of environment that is learnable 
  #ageWords <- round(wordsInWorld*y[lage]/y[length(y)])
  ageWords <- wordsInWorld
  #iis <- subgraph(ii, 1:ageWords) 
  # make training data set
  traindata <- c()
  # sample edges from environment
  for(i in 1:learningEvents){
   traindata <- rbind(traindata, cue_outcome <- ends(iis, sample(E(iis), 1, prob=E(iis)$weight))) 
  }
  # keep list of first words learned in year 1
  if(lage == 1){
    firsttraindata <- traindata
  }
  # train on cue-outcome pairs
  for(i in 1:learningEvents){
    cue_outcome <- traindata[i,]
    cue_outcome<- sample(as.vector(cue_outcome))
    vmat <- rescorlaWagner(vmat, cue=c(n, cue_outcome[1]), outcome=cue_outcome[2], beta = .02) 
  }
 # make undirected graph from representation 
  gle <- graph_from_adjacency_matrix(vmat, weighted=TRUE, diag=FALSE, mode = "undirected")
  # remove context cue
  gle <- igraph::delete_vertices(gle, n)
  # take subgraph of learnable words (only nec. if growing)
  gle <- igraph::subgraph(gle, 1:ageWords)
  nodeCount[lage] <- length(V(gle))
  # save learned representation
  ageNetworkList[[lage]] <- gle
  # copy network
  g2 <- gle
  # get symmetric weighted network 
  weightMatrix <- as_adjacency_matrix(g2, attr="weight", sparse=FALSE)
  # set negative values to zero
  weightMatrix[weightMatrix < 0] <- 0
  # normalize rows for entropy
  ww<-weightMatrix/rowSums(weightMatrix, na.rm=TRUE)
  # entropy function
  entropy <- function(p) rowSums(-(p * log(p)), na.rm = TRUE)
  # compute entropy for each node
  node_entropy <- entropy(ww) 
  
  # only compute entropy for words learned about at time 1 (removes isolates with 0 entropy)
  ftlist <- unique(c(firsttraindata[,1], firsttraindata[,2]))
  # take median entropy of list
  edgeE[lage] <- mean(node_entropy[ftlist])
  # remove edges with 0 or less weight
  g2 <- igraph::delete.edges(gle, which(E(gle)$weight <=0.0))
  # set remaining weights to 1
  E(g2)$weight <- 1
  # plot learned representation
  plot(g2, vertex.size = 1, edge.arrow.size = 0, vertex.label=NA, layout=layout_with_fr(g2))
  # label first one in 'learned'
  if(lage == 1){
    text(0, 1.5, "Learned lexicon")
  }
  # label all with iterations
  its <- lage*learningEvents
  text(0, -1.5, paste("t =",its))
}

```

```{r entropySim, fig.cap="The rising entropy and falling similarity of the aging lexicon as a function of learning. ",fig.height=3.4, eval = TRUE}

#### Set Plot Parameters ####
par(mfrow=c(1,2))
par(mar=c(5,5,2,2))

#### Plot Entropy ####

x <- 1:ageEpochs
plot(x, edgeE[1:ageEpochs], xlab = "Age", ylab = "Entropy", type = "b", cex.lab = 1.4)


#### Compute and Plot Similarity

# choose target pairs, from first training data (firsttraindata)
 learningPairs = 20 # this 50 random pairs
 simpair <- firsttraindata[sample(seq(1:nrow(firsttraindata)), learningPairs ),]
# simpair <- firsttraindata # this takes all pairs
# set initial activation levels
simpair <- data.frame(simpair, activation = 100)

# time for spreading activation
tspr <- 10 
# assign column names
names(simpair) <- c("node", "node", "activation")
## create datastore for similarity judgments
simJudge <- c()
## for each age network
for(sage in 1:length(ageNetworkList)){
## initiate activation at each node and measure activation at the other 
  for(testrow in 1:nrow(simpair)){
      # initiate activation at the cue
      df1 <- spreadr::spreadr(start_run = simpair[testrow,c(1,3)], decay = 0,
                                    retention = 0, suppress = 0,
                                    network = ageNetworkList[[sage]], time = tspr)
      # measure max activation at the target node
      maxActivation12 <- max(subset(df1, node == simpair[testrow,2])$activation)
      # initiate activation at the other cue
      df2 <- spreadr::spreadr(start_run = simpair[testrow,c(2,3)], decay = 0,
                                    retention = 0, suppress = 0,
                                    network = ageNetworkList[[sage]], time = tspr)
      # measure max activation at the target node
      maxActivation21 <- max(subset(df2, node == simpair[testrow,1])$activation)
      # add the max activations
      simval <- maxActivation12 + maxActivation21
      # add results to the data frame
      simJudge <- rbind(simJudge, c(simpair[testrow,1],simpair[testrow,2], simval, sage))
  }
}
# ready data frame with results
simJudge <- data.frame(simJudge)
# label columns
names(simJudge) <- c("node1", "node2", "similarity", "age")
# get stats by age
sed <- summarySE(simJudge, measurevar="similarity", groupvars="age")
# make x values for age
agex <- 1:ageEpochs
# plot similarity measurements across ages
plot(agex, sed$similarity[1:ageEpochs], xlab = "Age", ylab = "Similarity", type="b", cex.lab = 1.3)
```

# Discussion

Because edges were formed in @dubossarsky2017quantifying and @wulff2019new by requiring a minimun number of cue-target associations in the free association task, rising entropy corresponds to a less predictable pattern of cue-target associations and the lower likelihood of an edge between any cue-target pair. As a consequence, representational networks with higher entropy will produce behavioural networks that are more sparse.

Write some more stuff.

Do people who learn more showing earlier degradation? look at clutter paper.

@ramscar2014myth suggested that "older adults' changing performance reflects memory search demands, which escalate as experience grows" (p. 5) because older adults largely show impaired paired-associated learning only for unrelated terms. In subsequent work,

Borges-holthoefer hyperpriming (the opposite effect)

What do we mean by learning?  We don't mean higher education. 

As the authors note, "Experience is underappreciated as a factor in cognitive performance and is absent in most models of cognitive aging" (p. 118).


 By their account, proposing biological aging is uneccesary as the result is already explained by standard learning models. As @ramscar2017mismeasurement state, "the discriminative processes that produce 'associative' learning teaches English speakers not only which words go together, but also which words do not go together. This process both increasingly differentiates meaningful and meaningless word pairs and makes meaningless pairs harder to learn" (p. 3).


\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
